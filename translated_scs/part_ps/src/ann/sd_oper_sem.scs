.system_element_6137
=> nrel_inclusion: [*

	.system_element_6138
	=> nrel_note: [<p>Операционной семантикой любого языка представления методов решения задач является спецификация семейства агентов, обеспечивающих интерпретацию любого метода, принадлежащего соответствующему классу методов. Это семейство является интерпретатором соответствующего метода решения задач. В рамках технологии OSTIS такой интерпретатор называется моделью решения задач. Так как в рамках Технологии OSTIS используется многоагентный подход, то разработка нейросетевой модели решения задач сводится к разработке агентно-ориентированноймодели интерпретации и.н.с.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	=> nrel_note: [<p><b><i>Операционная семантика Языка представления нейросетевого метода в базах знаний</i></b> задается <i>многоагентный подход</i> к интерпретации <i>искусственных нейронных сетей</i> и спецификацией соответствующих действий.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	=> nrel_note: [<p>Нейросетевой метод описан в виде программы на некотором <i>языке программирования</i>, который может быть как внешним по отношению к <i>ostis-системе</i>, так и внутренним (на данный момент, <i>Язык SCP</i>). Каждому такому <i>языку программирования</i> соответствует некоторая дочерняя <i>предметная область</i> <i>Предметная область нейросетевых методов</i></p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
		=> .system_element_122: .system_element_5854;;
	*);
	=> nrel_note: [<p>В случае описания <i>нейросетевого метода</i> на внешнем языке, такой метод описывается в соответствующей предметной области, в рамках которой также специфицируется действие интерпретации данного метода. Данному действию соответствует агент, реализованный на соответствующем <i>языке программирования</i>.Однако для достижения конвергенции и интеграции необходимо описывать нейросетевые методы на внутреннем языке ostis-системы, которым является <i>Язык SCP</i>.Интерпретация <i>scp-программы</i> сводится к агентно-ориентированной обработке действий в sc-памяти. Этими действиями являются <i>scp-операторы</i>.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);;

	.system_element_6139
	=> nrel_idtf: [<p>Предметная область искусственных нейронных сетей</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	=> .system_element_4413: 
		.system_element_6140;
		.system_element_6141;
		.system_element_6142
	;;

	.system_element_6143
	=> .system_element_417: {
		.system_element_6144;
		.system_element_6145;
		.system_element_6146;
		.system_element_6147
	};
	=> nrel_note: [<p>При необходимости задавать различные аргументы для нейронов одного и того же слоя, можно специфицировать соответствующие действия, однако на данный момент этого не было произведено из-за слабой изученности подобного рода <i>нейросетевых моделей решения задач</i>.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);;

	.system_element_6148
	=> nrel_idtf: [<p>ормножество чисел</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	<= nrel_inclusion: number;
	<= nrel_inclusion: oriented_set;
	<= nrel_first_domain: .system_element_6149;
	=> nrel_note: [<p>Для описания спецификации указанных действий необходимо ввести понятия <i>ориентированного множества чисел</i> и <i>матрицы</i>, с помощью которых задаются входные значения <i>и.н.с.</i>, выходные значения <i>и.н.с.</i>, матрицы весовых коэффициентов и прочее.Каждый элемент ориентированного множества чисел является некоторым числом. Числа могут быть представлены в виде sc-узлов, либо с помощью строкового представления всего множества, для чего используется специальное отношение <i>строковое представление ормножества чисел*</i>, которое введено в целях оптимизации некоторых вариантов реализации агента, интерпретирующего действие, использующее понятие ориентированного множества чисел.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);;

	.system_element_6150
	=> nrel_note: [<p><i>матрица</i> является <i>ориентированным множеством</i> <i>ориентированных множеств</i> чисел равной мощности.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);;

	.system_element_6151
	=> nrel_note: [<p>Аргументы (<i>объекты'</i>) этого действия задаются следующими отношениями: <i>входной вектор'</i>, <i>матрица весовых коэффициентов нейронов слоя'</i>.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	-> nrel_result: .system_element_6152;;

	.system_element_6153
	=> nrel_first_domain: .system_element_5978;
	=> nrel_second_domain: .system_element_6148;;

	.system_element_6154
	=> nrel_first_domain: .system_element_6155;
	=> nrel_second_domain: .system_element_6150;;

	.system_element_6156
	=> .system_element_253: "file://action_weighted_sum.png"
	(*
		<- concept_file;;
		=> nrel_format: format_png;;
		=> nrel_note: [<p>Пример спецификации действия вычисления взвешенной суммы всех нейронов слоя для слоя с двумя нейронами и входным вектором размерностью 2</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;
	*);;

	.system_element_6157
	=> nrel_note: [<p>Аргументы этого действия задаются следующими отношениями: <i>вектор взвешенных сумм нейронов слоя'</i>, <i>вектор порогов нейронов слоя'</i>, <i>функция активации'</i>.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	-> nrel_result: .system_element_6158;;

	.system_element_6159
	=> nrel_first_domain: .system_element_6155;
	=> nrel_second_domain: .system_element_6148;;

	.system_element_6160
	=> nrel_first_domain: .system_element_6155;
	=> nrel_second_domain: .system_element_6148;;

	.system_element_6161
	=> nrel_first_domain: .system_element_6155;
	=> nrel_second_domain: .system_element_6023;
	=> nrel_note: [<p>Любой агент, интерпретирующий действия с заданными с помощью отношения <i>функция активации'</i> аргументами, должен использовать интерпретатор математических функций. использующихся в качестве функций активации.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);;

	.system_element_6162
	=> nrel_note: [<p>Аргументы этого действия задаются следующими отношениями: <i>входная матрица'</i>, <i>ядро свертки'</i>, <i>шаг свертки'</i>.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	-> nrel_result: .system_element_6163;;

	.system_element_6164
	=> nrel_first_domain: .system_element_5978;
	=> nrel_second_domain: .system_element_6150;;

	.system_element_6165
	=> nrel_first_domain: .system_element_6146;
	=> nrel_second_domain: .system_element_6150;;

	.system_element_6166
	=> nrel_first_domain: .system_element_6146;
	=> nrel_second_domain: number;;

	.system_element_6167
	=> nrel_note: [<p>Аргументы этого действия задаются следующими отношениями: <i>шаг окна пулинга'</i>, <i>размер окна пулинга'</i>, <i>входная матрица'</i></p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	-> nrel_result: .system_element_6168;;

	.system_element_6164
	=> nrel_first_domain: .system_element_5978;
	=> nrel_second_domain: .system_element_6150;;

	.system_element_6169
	=> nrel_first_domain: .system_element_6147;
	=> nrel_second_domain: .system_element_6150;;

	.system_element_6170
	=> nrel_first_domain: .system_element_6147;
	=> nrel_second_domain: number;;

	.system_element_6171
	=> nrel_note: [<p>Спецификация агентов, соответствующих указанным действиям, задает агентно-ориентированную модель интерпретации искусственных нейронных сетей. Реализация этой модели будет называться интерпретатором искусственных нейронных сетей</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	=> nrel_note: [<p>Реализация интерпретатора описанных в данной главе действий по построению <i>и.н.с.</i> и описания в базе знаний экспертных знаний разработчиков<i>и.н.с.</i> (а значит реализация интеллектуальной среды проектирования <i>и.н.с.</i>) позволит автоматически, исходя из описания задачи, генерировать нейросетевые методы в памяти <i>ostis-системы</i>, что является одним из ключевых направлений дальнейшего развития конвергенции и интеграции и.н.с. с базами знаний.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);;

	.system_element_6172
	=> nrel_note: [<p>Рассмотрим пример описания <i>нейросетевого метода</i>, решающего задачу, которая формулируется следующим образом: вычислить результат логической операции "ИСКЛЮЧАЮЩЕЕ ИЛИ" для значений двух логических переменных. На рисунке представлено решение этой задачи с помощью сигнальной функции.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);
	=> .system_element_253: "file://strong_or_graphic.png"
	(*
		<- concept_file;;
		=> nrel_format: format_png;;
	*);;

	.system_element_6173
	=> nrel_note: [<p>В работе описан однослойный персептрон, решающий поставленную задачу. Персептрон состоит из двух входных нейронов и одного выходного, с заданным порогом в 0,5 и сигнальной функцией активации:F(S) =1, 0 < S < 0,0, else</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
		=> .system_element_122: {
			.system_element_5853
		};;
	*);;

	.system_element_6174
	=> .system_element_253: "file://strong_or_ann.png"
	(*
		<- concept_file;;
		=> nrel_format: format_png;;
	*);;

	.system_element_6175
	=> .system_element_253: "file://exclusive_or_ann_scp.png"
	(*
		<- concept_file;;
		=> nrel_format: format_png;;
	*);;

	.system_element_6176
	=> .system_element_253: "file://signal_function_def.png"
	(*
		<- concept_file;;
		=> nrel_format: format_png;;
	*);
	=> nrel_note: [<p>Весовые коэффициенты синапсов входного слоя равны 1. На рисунке <i>Рисунок. Схема однослойного персептрона, решающего задачу "ИСКЛЮЧАЮЩЕЕ ИЛИ"</i> представлена схема персептрона.Данному персептрону соответствует метод, представленный в базе знаний ostis-системы на описанном в этой главе языке представления нейросетевых методов SCP. Данный метод представлен на рисунке <i>Рисунок. Метод, решающий задачу "ИСКЛЮЧАЮЩЕЕ ИЛИ", представленный с помощью языка представления нейросетевых методов SCP</i>.Описание метода состоит из последовательности двух обобщенных спецификаций действий --- действия вычисления взвешенной суммы всех нейронов слоя и действия вычисления функции активации для всех нейронов слоя.Сигнальная функция активации, использующаяся в персептроне, в памяти ostis-системы определяется логической формулой, представленной на рисунке <i>SCg-текст. Представление сигнальной функции активации в памяти ostis-системы</i>.</p>]
	(*
		<- lang_ru;;
		=> nrel_format: format_html;;
	*);;

	.system_element_4983
	=> nrel_inclusion: [*

		.system_element_6177
		-> .system_element_112: 
			.system_element_6178;
			.system_element_6179;
			.system_element_6180;
			.system_element_6181;
			.system_element_6182;
			.system_element_6183;
			.system_element_6184;
			.system_element_6185;
			.system_element_6186;
			.system_element_6187;
			.system_element_6188;
			.system_element_6189;
			.system_element_6190;
			.system_element_6191;
			.system_element_5980;
			.system_element_5982;
			.system_element_6012
		;;

		.system_element_6192
		<- .system_element_5250;
		=> nrel_note: [<p>Наличия <i>Языка представления нейросетевых методов в базах знаний</i> и его интерпретатора позволяет обеспечить интерпретацию <i>нейросетевого метода</i> в памяти <i>ostis-системы</i>. Наличие в единой памяти не только экземпляров методов, но и понятий, их описывающих, создает основу для автоматизации процесса построения нейросетевых методов. В памяти <i>ostis-системы</i> хранятся знания о том, методы какого класса могут решить задачу заданного класса, но экземпляров класса этого метода может не быть представлено в системе. На этот случай система должна иметь возможность сообщить пользователю о возможности решения, для которого, однако, необходимо погрузить в систему определенный метод. Так как система хранит в единой памяти задачу и требования к методу ее решения, появляется возможность спроектировать необходимый метод. Для этого необходимо наличие среды проектирования методов соответствующих классов. В случае <i>нейросетевого метода</i>, речь идет об интеллектуальной среде построения <i>нейросетевых методов</i>.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6193
		=> nrel_note: [<p>В основе интеллектуальной среды построения <i>нейросетевых методов</i> лежат соответствующие другу другу иерархии действий, задач и методов построения <i>и.н.с.</i> Наличие такой иерархии позволит описать язык представления методов построения <i>и.н.с.</i> и разработать интерпретатор этого языка.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>Построение иерархии соответствующих действий построения <i>и.н.с.</i> следует начать с изучения этапов проектирования и обучения <i>и.н.с.</i>, которые, в общем случае, выполняют все разработчики и.н.с.:. Постановка задачи2. Предобработка выборки: очистка3. Предобработка выборки: выявление содержательных признаков4. Предобработка выборки: трансформация5. Разбиение выборки на обучающую, валидационную и тестовую (контрольную)6. Выбор класса нейросетевых методов в соответствии со сформулированной задачей7. Формирование спецификации на входные и выходные данные8. Выбор метода оптимизации9. Выбор минимизируемой функции ошибки10. Начальная инициализация параметров нейронной сети11. Выбора гиперпараметров и.н.с.12. Обучение модели на обучающей выборке13. Оценка эффективности и.н.с</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6194
		=> nrel_note: [<p>Постановка задачи включает в себя описание входных данных (изображения/видео, временные ряды, текст), выходных данных и требований к методу решения (скорость, затраты по памяти и так далее). Также описывается дополнительная информация, которая может помочь в построении метода решения задачи (к примеру, спецификация обучающей выборки, если таковая имеется). Обычно, на данном этапе разработчик и.н.с. определяет класс задачи, формирует требования к обучающей выборке, если она не предоставлена.Выполнение данного этапа средой проектирования <i>и.н.с.</i> подразумевает выполнение следующих действий: <b><i>действие трансляции условия задачи</i></b>. Действие транслирует заданное с помощью <i>интерфейса ostis-системы</i> (к примеру, естественно-языкового интерфейса) описание задачи в память ostis-системы. Действие необходимо в случае, когда условие задачи задается пользователем. Необходимо понимать, что описание задачи поступает в базу знаний не только от <i>пользовательского интерфейса</i>. К примеру, задача может быть сформулирована самой системой в ходе ее жизнедеятельности.Данное действие является общим для всех ostis-систем, поэтому его рассмотрение выходит за рамки рассмотрения процесса построения интеллектуальной среды проектирования <i>и.н.с.</i>

		<li> <b><i>действие классификации задачи</i></b>. Действие определяет класс задачи (задача регрессии, детекции, кластеризации и так далее), исходя из описания задачи в базе знаний.

		<li> <b><i>действие поиска подходящей обучающей выборки</i></b>. В базе знаний может храниться набор спецификаций выборок, к которым у ostis-системы есть доступ. Действие производит поиск выборок, которые могут быть использованы в качестве обучающей выборки.

		<li> <b><i>действие формирования требований к обучающей выборке</i></b>. Если обучающая выборка не была предоставлена и не была найдена, то необходимо сформировать описание требований к обучающей выборке, которое можно будет транслировать на язык пользовательского интерфейса и запросить необходимую выборку у пользователя.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6195
		-> .system_element_6196
		(*
			=> nrel_note: [<p>На этом этапе обнаруживаются признаки, которые имеют в общем случае некорректные значения (например, для каких-то образов значение признака может иметь неопределенное значение, либо значение, не совпадающее по типу, либо аномально большое или очень маленькое значение, которое встречается в редком числе случаев). Для признаков, имеющих неопределенное значение, может быть применены различные методы устранения, например, такие значения могут быть заменены средним значением этого признака, рассчитанным по всем образам (для непоследовательных данных), либо они могут быть заменены средним значением по соседним образам (в случае временных рядов), либо каким-то фиксированным значением. Радикальная мера решения проблемы --- удаление образов, имеющих неопределенные значения признаков из выборки. Однако его лучше применять, если образов с отсутствующими значениями признаков немного. Для выбросов и аномалий применяются схожие стратегии (но только в том случае, если задача не состоит в прогнозировании этих аномалий).В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия очистки выборки</i></b>, которое выполняется в случае обработки выборки, которая ранее не была представлена в памяти системы (к примеру, была получена от пользователя).Реализация интерпретатора (агента) данного действия требует описания в памяти классификации стратегий очистки данных и реализации методов применения этих стратегий.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);;
		*);
		-> .system_element_6197
		(*
			=> nrel_note: [<p>Осуществляется инжиниринг признаков, состоящий в отборе признаков, влияющих на результат работы модели, несодержательные признаки, которые никак не коррелируют с выходом модели, удаляются. Цель этого этапа --- уменьшение размерности пространства признаков для снижения влияния эффекта переобучения на модель.Для снижения размерности признакового пространства может применяться методы отбора признаков и выделения признаков.При отборе признаков, осуществляется формирование подмножества из исходных признаков (алгоритм последовательного обратного отбора, рекурсивный алгоритм обратного устранения признаков,  алгоритмы с использованием случайных лесов).При выделении признаков из набора признаков извлекается информация для построения нового подпространства признаков (алгоритмы с использованием автоэнкодера).В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия выявления содержательных признаков</i></b>. Реализация интерпретатора (агента) данного действия требует описания в памяти классификации стратегий уменьшения размерности признакового пространства и реализации методов применения этих стратегий.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);;
		*);
		-> .system_element_6198
		(*
			=> nrel_note: <
				[<p>На этом этапе осуществляется подготовка данных к обучению.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);
				[<p>Здесь следует уделить особое внимание наличию категориальных признаков, чаще всего заданных строковыми типами. Эти признаки могут быть номинальными и порядковыми. Для кодирования порядковых признаков чаще всего применяют последовательный числовой код (1, 2, 3,...). Для кодирования номинальных такое решение неверно, так как эти признаки равноправны и не могут сравниваться по числовому коду (например, пол --- 0/1). Для номинальных признаков применяется способ прямого кодирования, заключающийся в создании и использовании фиктивных признаков по количеству значений исходного. Например, признак пол (мужской, женский) преобразуется в два новых признака мужской и женский с соответствующими значениями для имеющихся образов.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);
				[<p>Масштабирование признаков предполагает приведение значений признаков к одному общему интервалу --- это особенно актуально для признаков, имеющих несоразмерные выборочные средние значения по всем образам --- например, один признак в среднем имеет значение 10.000, а другой 12. Это может проявится в выполнении минимизации только по признаку с наибольшими значениями и плохой сходимости метода обучения. Чаще всего масштабирование соответствует выполнению нормализации на отрезок (min-max нормализация)</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
					=> .system_element_5971: .system_element_6199
					(*
						=> nrel_note: [<p>x<sup>i</sup> --- значение признака для отдельно взятого образа <i>i</i>, x<sub>min</sub> --- наименьшее значение для признака, x<sub>max</sub> --- наибольшее значение для признака.</p>]
						(*
							<- lang_ru;;
							=> nrel_format: format_html;;
						*);;
					*);;
				*);
				[<p>Другой вариант масштабирования --- применение стандартизации признаков</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
					=> .system_element_5971: .system_element_6200
					(*
						=> nrel_note: [<p>(x) --- выборочное среднее отдельного признака, (x) --- стандартное отклонение.</p>]
						(*
							<- lang_ru;;
							=> nrel_format: format_html;;
						*);;
					*);;
				*);
				[<p>Стандартизация сохраняет полезную информацию о выбросах в исходных данных и делает алгоритм обучения менее чувствительным к ним.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);
				[<p>Дискретизация применяется для перехода от вещественного признака к порядковому за счет кодирования интервалов одним значением (например, если признак отражает возраст человека, то может быть произведена дискретизация значений с выделением определенных возрастных групп, где каждая группа будет кодироваться одним целым числом).</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);
				[<p>В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия трансформации выборки</i></b>. Реализация интерпретатора (агента) данного действия требует описания в памяти классификации методов масштабирования признаков и реализации методов применения этих стратегий.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*)
			>;;
		*);;

		.system_element_6201
		=> nrel_note: [<p>Производится разбиение всей выборки данных, на обучающую, тестовую и, в некоторых случаях, валидационную.Валидационная выборка используется для оценки влияния изменения гиперпараметров на результат обучения и может применяться как дополнительный инструмент для этого наравне с сеточным поиском.Разбиение проводится в соотношении 3:1:1, в процентах (60/20/20), если валидационная выборка не используется, то 80/20.В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия разбиения выборки</i></b>.Все предыдущие этапы применялись к выборке, последующие этапы относятся к используемым моделям и.н.с.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6202
		=> nrel_note: [<p>На этом этапе осуществляется выбор основной архитектуры и.н.с., которая будет использоваться при обучении. Однако, нужно отметить, что этот выбор относительно условный, то есть исследователь не ограничен использованием только одного типа и.н.с. для решения задачи (как, например, сверточной сети для изображений, поскольку изображения можно обрабатывать и обычным многослойным персептроном). Речь скорее идет именно о рекомендованной архитектуре, но это не исключает использование любых других вариантов архитектур и их сочетаний в рамках одной модели).Примерами таких рекомендаций являются: изображения/видео --- сверхточные нейронные сети;

		<li> временные ряды --- многослойные персептроны или рекуррентные сети;

		<li> текстовая информация --- многослойные персептроны или рекуррентные сети;

		<li> наборы характеристик некоторых объектов (например, спецификации автомобилей) --- многослойный персептрон.В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия выбора класса нейросетевых методов</i></b>.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6203
		=> nrel_note: [<p>Выполняются дополнительные преобразования данных, связанные с изменением структур хранения (например, преобразование многомерного массива в одномерный, конвертация типов)В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия формирования спецификации входов и выходов и.н.с.</i></b>.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6204
		=> nrel_note: [<p>В рамках ПрО и.н.с. описаны следующие методы оптимизации: стохастический градиентный спуск (stochastic gradient descent --- SGD);

		<li> метод Нестерова;

		<li> адаптивный градиент (adaptive gradient --- AdaGrad);

		<li> адаптивная оценка момента (adaptive moment estimation --- Adam);

		<li> среднеквадратическое распространение (root mean square propagation --- RMSProp).В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия выбора метода оптимизации</i></b>.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6205
		=> nrel_note: <
			[<p>На этом этапе задается функция ошибок, которая будет минимизироваться. К примеру, MSE лучше подходит для задач регрессии и для кластеризации, CE --- для классификационных задач. Далее приведены примеры.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);
			[<p>MSE = <sup>1</sup>&frasl;<sub>n</sub> <sub>i=1</sub><sup>n</sup> (Y<sub>i</sub> - )<sup>2</sup></p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> nrel_note: [<p><i>n</i> --- размер обучающей выборки, Y<sub>i</sub> --- эталонное значение функции,  --- результат, полученный НС</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);;
			*);
			[<p>CE = - <sup>1</sup>&frasl;<sub>n</sub> <sub>i=1</sub><sup>n</sup> (Y<sub>i</sub>log() + (1-Y<sub>i</sub>)log(1 - ))</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> nrel_note: [<p><i>n</i> --- размер обучающей выборки, Y<sub>i</sub> --- эталонное значение функции,  --- результат, полученный НС.(случай 2-классовой классификации)</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);;
			*);
			[<p>CE = - <sup>1</sup>&frasl;<sub>n</sub> <sub>i=1</sub><sup>n</sup> <sub>c=1</sub><sup>M</sup> Y<sub>i</sub><sup>c</sup> log</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> nrel_note: [<p>случай многоклассовой классификации</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);;
			*);
			[<p>В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия выбора минимизируемой функции ошибки</i></b>.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*)
		>;;

		.system_element_6206
		=> .system_element_252: [<p>Наиболее часто используемые варианты инициализации весовых коэффициентов и порогов нейронной сети</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
			=> nrel_subdividing: {
				[<p>Инициализация значениями из равномерного распределения на каком-то небольшом интервале, например,  \[ -0.1, 0.1\].</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);
				[<p>Инициализация значениями из стандартного нормального распределения.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
				*);
				[<p>Инициализация по методу Ксавье.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
					=> .system_element_122: {
						.system_element_5855
					};;
					=> nrel_note: [<p>Применяется для предотвращения резкого уменьшения или увеличения значений выхода нейронных элементов после применения функции активации при прямом прохождении образа через глубокую нейронную сеть. Фактически инициализация этим методом осуществляется посредством выбора значений из равномерного распределения на отрезке \[ -  / ,  / \] , где n<sub>i</sub> --- это число входящих связей в данный слой, а n<sub>i</sub> --- число исходящих связей из данного слоя. Таким образом, инициализация этим методом проводится для разных слоев нейронной сети из разных отрезков.</p>]
					(*
						<- lang_ru;;
						=> nrel_format: format_html;;
					*);;
				*);
				[<p>Инициализация, полученная из предобученной модели.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
					=> .system_element_122: {
						.system_element_5855
					};;
					=> nrel_note: [<p>Вариант инициализации, который предполагает использование в качестве "стартовой" модели предобученной модели, взятой из некоторого репозитория предобученных моделей, обученную самим исследователем или в процессе работы интеллектуальной системы.</p>]
					(*
						<- lang_ru;;
						=> nrel_format: format_html;;
					*);;
				*);
				[<p>Инициализация по методу Кайминга.</p>]
				(*
					<- lang_ru;;
					=> nrel_format: format_html;;
					=> .system_element_122: {
						.system_element_5856
					};;
					=> nrel_note: [<p>Данный метод инициализации применяется для решения проблемы "затухающего" градиента и "взрывающегося" градиента. Производится посредством выбора значений из равномерного распределения на отрезке: \[ - / ,  / \]где <i>a</i> --- угол наклона к оси абсцисс для отрицательной части области определения функции активации типа ReLU (для обычной ReLU функции этот параметр равен 0), fan --- параметр режима работы, который для фазы прямого распространения равен количеству входящих связей (для устранения эффекта "взрывающегося" градиента), а для фазы обратного распространения --- количеству выходящих (для устранения эффекта "затухающего" градиента).В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия начальной инициализации и.н.с.</i></b>.</p>]
					(*
						<- lang_ru;;
						=> nrel_format: format_html;;
					*);;
				*)
			};;
		*);;

		.system_element_6207
		=> nrel_note: [<p>На практике некоторые гиперпараметры (такие как количество слоев, их типы, количество нейронов в слое) часто определяются экспериментально, в процессе итеративного поиска лучшего варианта решения задачи. Хотя способы частично автоматизировать этот процесс существуют, они все же рассчитаны на наличие некоторых предусловий проведения эксперимента, в частности интервалов изменения параметра (например, скорости обучения).К гиперпараметрам, подбираемым на этом этапе, относятся: параметры обучения <i>и.н.с.</i> (скорость обучения, моментный параметр, размер мини-батча);

		<li> архитектура модели <i>и.н.с.</i>, опирающаяся на ранее сформулированные спецификации входных и выходных данных (например, количество нейронов в определенном слое (слоях) или конфигурации целых слоев).Нахождение оптимальных гиперпараметров может быть получено, например, использованием метода сеточного поиска, который позволяет проверить значения гиперпараметров, взятые с определенным шагом или из определенного интервала (кортежа). С помощью этого метода выбирается оптимальный набор гиперпараметров, который дает лучшие результаты, он используется для последующего дообучения. Или же, если полученные результаты являются приемлемыми, то процесс дальнейшего обучения вообще не проводится. Следует отметить затратность данного метода, так как фактически осуществляется перебор различных значений параметров обучения. Для снижения объема работы применяется метод случайного поиска.Для оптимизации архитектуры определяются типы слоев нейронной сети, количество нейронных элементов в каждом слое, их характеристики --- функция активации, для сверточных элементов --- размер ядра, а также параметры padding и шаг свертки (stride).Здесь же может осуществляться оценка не только пользовательского варианта сети, но и предобученной архитектуры. Основное правило при выборе --- количество параметров модели не должно превышать размер обучающей выборки. Для предобученных архитектур это ограничение снимается.В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия выбора гипперпараметров и.н.с.</i></b>. Действие использует классификацию и спецификации гиперпараметров <i>и.н.с.</i></p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6208
		=> nrel_note: [<p>Производится обучение модели до достижения выбранной точности (оценивается на тестовой выборке) или по другим заданным критериям (достижение заданного количества эпох обучения, неизменность точности на протяжении заданного количества эпох, падение точности на валидационной выборке и так далее).</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>В интеллектуальной среде проектирования данный этап соответствует выполнению <b><i>действия обучения и.н.с.</i></b>. Действие обучения <i>и.н.с.</i> --- действие, в ходе которого реализуется определенный метод обучения <i>и.н.с.</i> с заданными параметрами обучения <i>и.н.с.</i>, методом оптимизации и функцией потерь.При обучении возможно возникновение следующих проблем: <i>переобучение</i> --- проблема, возникающая при обучении <i>и.н.с.</i>, заключающаяся в том,что сеть хорошо адаптируется к паттернам входной активности из обучающей выборки, при этом теряя способность к обобщению.Переобучение возникает из-за применения неоправданно сложной модели при обучении <i>и.н.с.</i> Это происходит,когда количество настраиваемых параметров <i>и.н.с.</i> намного больше размера обучающей выборки. Возможныеварианты решения проблемы заключаются в упрощении модели, увеличении выборки, использовании регуляризации(параметр регуляризации, техника dropout и так далее).Обнаружение переобученности сложнее, чем недообученности. Как правило, для этого применяетсякросс-валидация на валидационной выборке, позволяющая оценить момент завершения процесса обучения.Идеальным вариантом является достижение баланса между переобученностью и недообученностью.
		<li> <i>недообучение</i> --- проблема, возникающая при обучении  <i>и.н.с.</i>, заключающаяся в том,что сеть дает одинаково плохие результаты на обучающей и контрольной выборках.Чаще всего такого рода проблема возникает при недостаточном времени, затраченном на обучение модели.Однако это может быть вызвано и слишком простой архитектурой модели либо малым размером обучающейвыборки. Соответственно решение, которое может быть принято ML-инженером, заключается в устраненииэтих недостатков: увеличение времени обучения, использование модели с большим числом настраиваемыхпараметров, увеличение размера обучающей выборки, а также уменьшение регуляризации и более тщательныйотбор признаков для обучающих примеров.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_5979
		<= nrel_inclusion: .system_element_465;
		=> nrel_subdividing: .system_element_6209
		(*
			<=> .system_element_200: {
				.system_element_5980
				(*
					=> nrel_explanation: [<p><b><i>метод обучения с учителем</i></b> --- метод обучения с использованием заданных целевых переменных.</p>];;
					=> nrel_inclusion: .system_element_5981
					(*
						=> nrel_idtf: [<p>м.о.р.о.</p>];;
						=> nrel_explanation: [<p>м.о.р.о. использует заданный метод оптимизации и заданную функцию потерь для реализации фазы обратного распространения ошибки и изменения настраиваемых параметров и.н.с. Одним из самых распространенныхметодов оптимизации является метод стохастического градиентного спуска.</p>]
						(*
							<- lang_ru;;
							=> nrel_format: format_html;;
						*);;
						=> nrel_explanation: [<p>Следует также отметить, что несмотря на то, что метод отнесен к методам обучения с учителем, в случаеиспользования м.о.р.о. для обучения автокодировщиков в классических публикациях он рассматривается какметод обучения без учителя, поскольку в данном случае размеченные данные отсутствуют.</p>]
						(*
							<- lang_ru;;
							=> nrel_format: format_html;;
						*);;
					*);;
				*);
				.system_element_5982
				(*
					=> nrel_explanation: [<p><b><i>метод обучения без учителя</i></b> --- метод обучения без использования заданных целевых переменных(в режиме самоорганизации)</p>];;
					=> nrel_explanation: [<p>В ходе выполнения алгоритма метода обучения без учителя выявляются полезные структурные свойства набора. Неформально его понимают как метод для извлечения информации из распределения, выборка для которогоне была вручную аннотирована человеком.</p>]
					(*
						<- lang_ru;;
						=> nrel_format: format_html;;
						=> .system_element_122: {
							.system_element_5857
						};;
					*);;
				*)
			};;
		*);;

		.system_element_6210
		=> .system_element_1120: [<p>метод обучения <i>и.н.с.</i> --- это процесс итеративного поиска оптимальных значений настраиваемых параметров <i>и.н.с.</i>, минимизирующих некоторую заданную функцию потерь.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>Стоит отметить, что хотя целью применения метода обучения является минимизация функции потерь, "полезность" полученной после обучения модели можно оценить только по достигнутому уровню ее обобщающей способности.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>Методы обучения могут быть поделены на две большие группы --- <i><b>методы обучения с учителем</b></i> и <i><b>методы обучения без учителя</b></i> (контролируемый и неконтролируемый методы обучения).<i>метод обучения с учителем</i> --- метод обучения с использованием заданных целевых переменных.Одним из методов обучения с учителем является метод обратного распространения ошибки.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_5981
		=> .system_element_6211: [<p>Приведем его описание в виде алгоритма:инициализация весов <i>W</i> и порогов <i>T</i>;<i>метод обратного распространения ошибки</i> использует заданный метод оптимизации и заданную функцию потерь для реализации фазы обратного распространения ошибки и изменения настраиваемых параметров и.н.с. Одним из самых распространенных методов оптимизации является метод стохастического градиентного спуска. Приведенный метод используется для реализации последовательного варианта обучения.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>Следует также отметить, что несмотря на то, что метод отнесен к методам обучения с учителем, в случае его использования для обучения автокодировщиков в классических публикациях он рассматривается как метод обучения без учителя, поскольку в данном случае размеченные данные отсутствуют.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_5982
		<- .system_element_6212;
		=> .system_element_1120: [<p>метод обучения без учителя --- это метод обучения без использования заданных целевых переменных (в режиме самоорганизации)</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>В ходе выполнения алгоритма метода обучения без учителя выявляются полезные структурные свойства набора. Неформально его понимают как метод для извлечения информации из распределения, выборка для которого не была вручную аннотирована человеком. Метод обучения без учителя может рассматриваться как вспомогательный метод для начальной инициализации настраиваемых параметров и.н.с. В этом случае он является методом предобучения.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
			=> .system_element_122: {
				.system_element_5857
			};;
		*);;

		.system_element_6213
		=> .system_element_6214: <
			[<p>SGD (стохастический градиентный спуск). В данном методе корректировка настраиваемых параметров и.н.с. выполняется в направлении максимального уменьшения функции стоимости, то есть в направлении, противоположном вектору градиента функции потерь.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_122: {
					.system_element_5853;
					.system_element_5858
				};;
			*);
			[<p>Метод Нестерова. Обучение методом стохастического градиентного спуска не редко происходит очень медленно. Импульсный метод позволяет ускорить обучение, особенно в условиях высокой кривизны, небольших, но устойчивых градиентов или зашумленных градиентов. В импульсном методе вычисляется экспоненциально затухающее скользящее среднее прошлых градиентов и продолжается движение в этом направлении. Метод Нестерова является вариантом импульсного алгоритма, в котором градиент вычисляется после применения текущей скорости.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_122: {
					.system_element_5857
				};;
			*);
			[<p>AdaGrad: данный метод по отдельности адаптирует скорости обучения всех настраиваемых параметров и.н.с., умножая их на коэффициент, обратно пропорциональный квадратному корню из суммы всех прошлых значений квадрата градиента.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_122: {
					.system_element_5859
				};;
			*);
			[<p>RMSProp. Данный метод является модификацией AdaGrad, которая позволяет улучшить его поведение в невыпуклом случае путем изменения способа агрегирования градиента на экспоненциально взвешенное скользящее среднее. Использование экспоненциально взвешенного скользящего среднего гарантирует повышение скорости сходимости после обнаружения выпуклой впадины, как если бы внутри этой впадины алгоритм AdaGrad был инициализирован заново</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_122: {
					.system_element_5857
				};;
			*);
			[<p>Adam. Данный метод можно рассматривать как комбинацию RMSProp и AdaGrad. Помимо усредненного первого момента, данный метод использует усредненное значение вторых моментов градиентов.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_122: {
					.system_element_5860
				};;
			*)
		>;
		=> nrel_note: [<p>Отметим, что успешность применения методов оптимизации зависит главным образом от знакомства пользователя с соответствующим алгоритмом.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
			=> .system_element_122: {
				.system_element_5857
			};;
		*);;

		.system_element_5984
		=> nrel_note: [<p>важный компонент, влияющий на процесс обучения нейросетевой модели</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> .system_element_1120: [<p>функция потерь --- это функция, используемая для вычисления ошибки, рассчитываемой как разница между фактическим эталонным значением и прогнозируемым значением, получаемым <i>и.н.с.</i></p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: <
			[<p>Среди функций потерь, используемые в качестве целевых функций для применяемого метода оптимизации, можно выделить MSE, BCE, MCE</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);
			[<p>MSE --- средняя квадратичная ошибка</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_5971: .system_element_6215
				(*
					=> nrel_note: [<p>y<sub>i</sub><sup>l</sup> --- прогноз модели, e<sub>i</sub><sup>l</sup> --- ожидаемый (эталонный) результат, <i>m</i> --- размерность выходного вектора, <i>L</i> --- объем обучающей выборки</p>]
					(*
						<- lang_ru;;
						=> nrel_format: format_html;;
					*);;
				*);;
			*);
			[<p>BCE --- бинарная кросс-энтропия (binary cross-entropy)</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_5971: .system_element_6216
				(*
					=> nrel_note: [<p>y<sup>l</sup> --- прогноз модели, e<sup>l</sup> --- ожидаемый (эталонный) результат: <i>0</i> или <i>1</i>, <i>L</i> --- объем обучающей выборки</p>]
					(*
						<- lang_ru;;
						=> nrel_format: format_html;;
					*);;
				*);;
			*);
			[<p>MCE --- мультиклассовая кросс-энтропия (multiclass cross-entropy)</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
				=> .system_element_5971: .system_element_6217
				(*
					=> nrel_note: [<p>y<sub>i</sub><sup>l</sup> --- прогноз модели, e<sub>i</sub><sup>l</sup> --- ожидаемый (эталонный результат), <i>m</i> --- размерность выходного вектора</p>]
					(*
						<- lang_ru;;
						=> nrel_format: format_html;;
					*);;
				*);;
			*)
		>;;

		.system_element_6218
		=> nrel_idtf: [<p>BCE</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>Отметим, что для бинарной кросс-энтропии в выходном слое <i>и.н.с.</i> будет находиться один нейрон, а для для мультиклассовой кросс-энтропии количество нейронов в выходном <i>слое и.н.с.</i> совпадает с количеством классов.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_5955
		=> nrel_note: [<p>Для решения задачи классификации рекомендуется использовать бинарную или мультиклассовую кросс-энтропийную функцию потерь, для решения задачи регрессии рекомендуется использовать среднюю квадратичную ошибку.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6219
		=> .system_element_253: "file://ann_training_nn_scg.png"
		(*
			<- concept_file;;
			=> nrel_format: format_png;;
			=> nrel_note: [<p>Пример действия обучения <i>и.н.с.</i></p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);;
		*);;

		.system_element_6220
		=> nrel_note: [<p>После выполнения обучения осуществляется оценка полученной модели с помощью метрик оценки качества.Далее результат оценки может быть визуализирован с помощью матрицы ошибок (confusion matrix) и ROC-кривой.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> nrel_note: [<p>В интеллектуальной среде проектирования данный этап соответствует выполнению <i>действия оценки эффективности и.н.с.</i>.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6221
		=> .system_element_1120: [<p>матрица ошибок --- это матрица, в которую помещены сведения о числе истинно-положительных, истинно-отрицательных, ложно-положительных и ложно-отрицательных предсказаниях классификатора.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> .system_element_180: .system_element_6222;;

		.system_element_6222
		=> .system_element_253: "file://conf_matrix.png"
		(*
			<- concept_file;;
			=> nrel_format: format_png;;
		*);;

		.system_element_6223
		=> nrel_idtf: [<p>receiver operating characteristic</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> .system_element_1120: [<p>ROC-кривая --- это график, в котором, основываясь на заданном пороге решения классификатора, рассчитываются доли ложноположительных и истинно положительных исходов. Основываясь на ROC-кривой, высчитывается AUC-показатель (площадь под кривой), которая используется в качестве характеристики качества модели.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6224
		=> .system_element_252: <
			[<p>Рассмотрим пример выполнения описанных этапов разработчиком для конкретной задачи --- <i>классификации цифр из выборки рукописных цифр MNIST</i>: Исходными данными задачи является: выборка из 70.000 изображений, предварительно разделенная на обучающую (60.000 изображений) и контрольную (10.000 изображений) выборки. Каждое изображение представлено двумерным массивом 28x28 чисел из интервала \[  0, 255\], числа представляют определенный оттенок серого цвета. Помимо этого каждому изображению соответствует метка класса, соответствующая конкретной цифре от 0 до 9.Ставится задача: <i>обучить модель, которая будет принимать на вход двумерный массив данных и возвращать метку класса, соответствующей распознанной цифре.</i>Таким образом, тип решаемой задачи --- <b>классификационная</b>, природа данных задачи --- <b>изображения</b>.
			<li> В рассматриваемой выборке отсутствуют аномалии, ошибочные данные, признаки с отсутствующими значениями.
			<li> В рассматриваемой задаче отсутствуют несодержательные признаки.
			<li> В качестве метода предобработки данных используем масштабирование признаков, а именно нормализацию на отрезок \[ 0, 1\].
			<li> Выполним разбиение обучающей части данных на обучающую и валидационную выборки в соотношении 4:1 (48.000 в обучающей и 12.000 в валидационной).
			<li> Так как выборка включает в себя изображения, будем использовать сверточную нейронную сеть.
			<li> Не требуется.
			<li> В качестве оптимизационного алгоритма будем использовать метод стохастического градиентного спуска (SGD).
			<li> Так как решается задача классификации, выберем в качестве минимизируемой функции кросс-энтропийную функцию потерь.
			<li> В качестве начальной инициализации будем использовать инициализацию по методу Кайминга.
			<li> На предыдущих этапах было определено, что для решения задачи будет использоваться сверточная нейронная сеть. При использовании one-hot кодирования в последнем полносвязном слое будет 10 нейронов по числу классов в задаче.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);
			[<p>Для упрощения будем использовать архитектуру, изображенную на <i>Рисунок. Архитектура и.н.с., решающая задачу классификации цифр</i>, не содержащую промежуточные слои.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);
			[<p>Для нахождения оптимального набора гиперпараметров будем применять метод случайного поиска.Перечислим кортежи, из которых будут сэмплироваться гиперпараметры: Скорость обучения --- (0.9, 0.1, 0.01, 0.001);
			<li> Количество нейронов в сверточном слое --- (5, 10, 15, 20);
			<li> Размер ядра свертки --- (3, 5, 7, 9);
			<li> Моментный параметр --- (0, 0.5, 0.9);
			<li> Размер мини-батча --- (16, 32, 64, 128).</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);
			[<p>После определения данных параметров и оценки эффективности работы алгоритма, получим следующую таблицу: <i>Таблица. Результаты решения задачи</i></p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);
			[<p>Можно заметить, что лучший результат (acc = 0.9839) по обобщающей способности на валидационной выборке был получен при следующих параметрах: mbs = 64, ks = 7, lr = 0.01, momentum = 0.9, cnc = 15. В качестве критерия останова нами был выбран самый простой критерий по достижению заданного количества эпох обучения. Дообучение не проводилось, для оценки обобщающей способности использовалась модель, полученная после выполнения процедуры подбора гиперпараметров. Обобщающая способность на тестовой выборке составила <b>0.9853</b>, то есть <b>98.53%</b>.
			<li> Построив матрицу ошибок на основании обученной модели и тестовой выборки, получим результат, проиллюстрированный на рис. <i>Рисунок. Матрица ошибок для задачи MNIST</i>Мы получили матрицу с явно выраженным диагональным преобладанием, таким образом полученная модель делает относительно небольшое число ошибок.</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*)
		>;;

		.system_element_6225
		=> .system_element_253: "file://model.png"
		(*
			<- concept_file;;
			=> nrel_format: format_png;;
		*);;

		.system_element_6226
		=> .system_element_253: "file://results_table.png"
		(*
			<- concept_file;;
			=> nrel_format: format_png;;
			=> nrel_explanation: [<p>Используемые сокращения: mbs --- mini-batch size, ks --- kernel size, lr --- learning rate, cnc --- convolutional neurons count, acc --- accuracy, it --- iterations count</p>]
			(*
				<- lang_ru;;
				=> nrel_format: format_html;;
			*);;
		*);;

		.system_element_6227
		=> .system_element_253: "file://conf_matrix_result"
		(*
			<- concept_file;;
			=> nrel_format: format_png;;
		*);;

		.system_element_6228
		=> nrel_note: [<p>Исходя из анализа этапов построения и.н.с., которые выполняют разработчики, можно вывести следующую классификацию действий по построению и.н.с.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		=> .system_element_417: {
			.system_element_6229
			(*
				=> .system_element_417: {
					.system_element_6180;
					.system_element_6181;
					.system_element_6182;
					.system_element_6183;
					.system_element_6184;
					.system_element_6185
				};;
			*);
			.system_element_6230
			(*
				=> .system_element_417: {
					.system_element_6186;
					.system_element_6187
				};;
			*);
			.system_element_6012
			(*
				=> .system_element_417: {
					.system_element_6188;
					.system_element_6189;
					.system_element_6190;
					.system_element_6191;
					.system_element_6012;
					.system_element_6231
				};;
			*)
		};;

		.system_element_6232
		=> nrel_note: [<p>Так как в результате действий по построению <i>и.н.с.</i> объект этих действий, конкретная <i>и.н.с.</i>, может существенно меняться (меняется конфигурация сети, ее весовые коэффициенты), то <i>и.н.с.</i> представляется в базе знаний как темпоральное объединение всех ее версий. Каждая версия является <i>и.н.с.</i> и темпоральной сущностью. На множестве этих темпоральных сущностей задается темпоральная последовательность с указанием первой и последней версии. Для каждой версии описываются специфичные знания.Общие для всех версий знания описываются для <i>и.н.с.</i>, являющейся темпоральным объединением всех версий (рисунок <i>SCg-текст. Темпоральность нейронной сети</i>)</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);;

		.system_element_6233
		=> .system_element_253: "file://temporal_neural_network_scg.png"
		(*
			<- concept_file;;
			=> nrel_format: format_png;;
		*);;
	*];
	=> nrel_conclusion: <
		[<p>В главе описан подход к <i>интеграции и конвергенции искусственных нейронных сетей с базами знаний</i> в <i>интеллектуальных компьютерных системах нового поколения</i> с помощью представления и интерпретации <i>искусственной нейронной сети</i> в <i>базе знаний</i>.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		[<p>Описаны <i>Синтаксис, Денотационная и Операционная семантика Языка представления нейросетевых методов в базах знаний</i>, который позволяет представить и интерпретировать в памяти интеллектуальной системы любую <i>и.н.с.</i> Наличие такого языка порождает семантическую совместимость нейросетевого метода с другими методами, представленными в памяти системы, что позволяет анализировать саму <i>и.н.с.</i> и этапы ее работы любыми другими методами системы.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		[<p>Так же наличие языка представления нейросетевых методов позволяет описывать в памяти системы экспертные знания разработчиков <i>и.н.с.</i> В главе приведены этапы построения <i>и.н.с.</i>, которые выполняют разработчики <i>и.н.с.</i> На основании этих этапов, c целью проектирования интеллектуальной среды построения <i>нейросетевых методов</i>, в <i>базе знаний</i> были классифицированы и описаны действия по построению <i>и.н.с.</i></p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		[<p>Проектирования и реализация интеллектуальной среды построения <i>и.н.с.</i> в <i>базе знаний</i> системы является одним из двух основных направлений дальнейшего развития работу по конвергенции и интеграции и.н.с. с базами знаний.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*);
		[<p>Вторым основным направлением является разработка подхода к обработке фрагментов <i>базы знаний</i> с помощью <i>и.н.с.</i>, для чего необходимо разработать универсальный алгоритм взаимно-однозначного соответствия фрагментов базы знаний и входных векторов <i>и.н.с.</i> Язык представления знаний способен представить любое знание. Наличие в системе нейросетевого метода, способного принимать на вход фрагменты знаний, позволит решить новые, слабо изученные классы задач.</p>]
		(*
			<- lang_ru;;
			=> nrel_format: format_html;;
		*)
	>;;
*];;
