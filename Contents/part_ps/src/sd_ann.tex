\begin{SCn}
\scnsectionheader{Предметная область и онтология sc-моделей искусственных нейронных сетей}
\begin{scnsubstruct}

\begin{scnrelfromlist}{соавтор}
	\scnitem{Головко В.А.}
	\scnitem{Ковалёв М.В.}
	\scnitem{Крощенко А.А.}
	\scnitem{Михно Е.В.}
\end{scnrelfromlist}

\begin{scnreltovector}{конкатенация сегментов}
	\scnitem{Предметная область и онтология искусственных нейронных сетей}
	\scnitem{Предметная область и онтология действий по обработке искусственных нейронных сетей}
\end{scnreltovector}

\scntext{аннотация}{Рассмотрен подход к интеграции и конвергенции искусственных нейронных сетей с базами знаний	в интеллектуальных компьютерных системах нового поколения с помощью представления и интерпретации искусственных нейронных сетей в базе знаний. Описаны \textit{Синтаксис, Денотационная и Операционная семантика Языка представления нейросетевых методов в базах знаний}. Описаны этапы построения нейросетевых методов решения задач с помощью интеллектуальной среды проектирования искусственных нейронных сетей.}

\begin{scnrelfromlist}{ключевой знак}
	\scnitem{Язык представления нейросетевых методов решения задач в базах знаний}
\end{scnrelfromlist}

\begin{scnhaselementrolelist}{класс объектов исследования}
	\scnitem{нейросетевой метод решения задач}
	\scnitem{нейросетевая модель решения задач}
	\scnitem{навык решения задач с помощью искусственных нейронных сетей}
	\scnitem{действие по построению искусственных нейронных сетей}
\end{scnhaselementrolelist}

\begin{scnrelfromset}{библиографическая ссылка}
	\scnitem{Castelvecchi2016}
	\scnitem{Ribeiro2016}
	\scnitem{Lundberg2017}
	\scnitem{Garcez2015}
	\scnitem{Besold2017}
	\scnitem{Golovko2019}
	\scnitem{Kroshchanka2022}
	\scnitem{Golovko2017}
	\scnitem{Kovalev2022}
	\scnitem{Glorot2010}
	\scnitem{He2015}
	\scnitem{Goodfellow2017}
	\scnitem{Haykin2006}
	\scnitem{Duchi2011}
	\scnitem{Kingma2014}
\end{scnrelfromset}

\begin{scnrelfromvector}{введение}
	\scnfileitem{В последнее десятилетие обозначилась устойчивая тенденция широкого применения методов машинного обучения в самых разных областях человеческой деятельности, обусловленная в первую очередь развитием теории искусственных нейронных сетей (и. н. с.), а также аппаратных возможностей.}
	\scnfileitem{Современные решатели задач интеллектуальных систем все чаще сталкиваются с необходимостью решения комплексных задач с помощью различных традиционных и интеллектуальных методов решения задач в едином информационном ресурсе (в пределе --- в единой базе знаний).}
	\scnfileitem{С другой стороны, интеллектуальные компьютерные системы нового поколения обладают, среди прочих, следующими способностями:
		\begin{itemize}
			\item способность постоянно повышать качество решения задач;
			\item способность приобретать навыки решения принципиально новых задач;
			\item способность обосновывать свои решения;
			\item способность находить и устранять ошибки в своих решения (способность к интроспекции).
		\end{itemize}}
	\scnfileitem{Представление различных методов решения задач в единой базе знаний обеспечивает семантическую совместимость этих методов. Решая задачу с помощью таких методов, система не взаимодействует с ними по принципу \scnqq{входов-выходов}. Напротив, единая память позволяет отслеживать преобразование входных знаний в реальном времени с помощью любых имеющихся методов, что обеспечивает способность к интроспекции и способность объяснять решения системы.}
	\scnfileitem{Перспективными и активно развивающимися методами решения задач являются искусственные нейронные сети (и.н.с.), что обуславливается, с одной стороны, развитием теории и.н.с., а с другой --- аппаратных возможностей машин, которые используются для их обучения.}
	\scnfileitem{Преимущество и.н.с. заключается в том, что они могут работать с неструктурированными данными.}
	\scnfileitem{Достоинствами и.н.с. можно назвать способность решения задач при неизвестных закономерностях, а так же способность решения задач без необходимости разработки проблемоориентированных подходов.}
	\scnfileitem{Главный недостаток и.н.с. --- это отсутствие понятной человеку обратной связи, которую можно было бы назвать цепочкой рассуждений, т.е. можно сказать, что и.н.с. работают как \textit{черный ящик}.
		\\Еще одним недостатком и.н.с. можно назвать эвристический характер процесса подбора архитектур моделей и 		параметров их обучения и высокие требования к объему знаний проектировщиков нейросетевых моделей.}
	\begin{scnindent}
		\begin{scnrelfromset}{источник}
			\scnitem{\scncite{gastelvecchi2016}}
		\end{scnrelfromset}
	\end{scnindent}
	\scnfileitem{Современные задачи все чаще требуют обоснования своего решения. Появилось целое направление Explainable AI, в рамках которого предпринимаются различные попытки объяснить решения и.н.с. Развиваются подходы, предлагающие интеграцию нейронных сетей с базами знаний.}
	\begin{scnindent}
		\begin{scnrelfromset}{источник}
			\scnitem{\scncite{Lundberg2017}}
			\scnitem{\scncite{Ribeiro2016}}
			\scnitem{\scncite{Lundberg2017}}
			\scnitem{\scncite{Garcez2015}}
			\scnitem{\scncite{Besold2017}}
			\scnitem{\scncite{Golovko2019}}
			\scnitem{\scncite{Kroshchanka2022}}
		\end{scnrelfromset}
	\end{scnindent}
	\scnfileitem{Сложность современных интеллектуальных систем, использующих нейросетевые модели, а также большой объём обрабатываемых ими данных обуславливают необходимость мониторинга, объяснения и понимания механизмов их работы с целью вербализации оценки и оптимизации их деятельности.}
	\scnfileitem{Исходя из перечисленных способностей, наличие которых необходимо обеспечивать в интеллектуальных компьютерных системах нового поколения, встает проблема разработки подхода к интеграции и.н.с. в базу знаний интеллектуальной системы как в качестве метода решения задач, так и в качестве объекта автоматического проектирования новых методов. Решение этой проблемы позволит преодолеть указанные выше недостатки нейросетевого	метода.}
	\scnfileitem{В связи с этим становится актуальна разработка нейросимволических подходов, в частности, подходов по интеграции и.н.с и баз знаний, использующих онтологии. Такие интегрированные системы способны сочетать:
		\begin{itemize}
			\item возможность семантической интерпретации обрабатываемых данных, используя представление решаемых и.н.с. прикладных задач, а так же спецификацию её входных и выходных данных;
			\item с представлением самой структуры и.н.с., описанием её свойств и состояний, позволяющими упростить понимание её работы.
	\end{itemize}}
	\begin{scnindent}
		\begin{scnrelfromset}{источник}
			\scnitem{\scncite{ann_ostis2018}}
			\scnitem{\scncite{nesy1}}
		\end{scnrelfromset}
	\end{scnindent}
	\scnfileitem{Можно выделить два основных направления интеграции и.н.с. с базами знаний:
		\begin{itemize}
			\item Построение интеллектуальных систем, способных использовать нейросетевые методы наравне с другими имеющимися в системе методами для решения задач или подзадач системы. Такие системы смогут учитывать семантику решаемых задач на более высоком уровне, что сделает решение этих задач более структурированными и прозрачными.
			\item Построение интеллектуальной среды по разработке, обучению и интеграции различных и.н.с., совместимых с базами знаний через представление и.н.с. с помощью онтологических структур и их интерпретацию средствами представления знаний. Такая среда предоставит возможность интроспекции и.н.с, возможность сохранения состояний и.н.с. после обучения и реконфигурации сети. Это позволит производить более глубокий анализ работы и.н.с. Так же формальное описание знаний рамках предметной области и.н.с. поможет поможет уменьшить порог вхождения разработчиков в методы решения задач с помощью и.н.с.\\
			Данный раздел посвящен предметной области и онтологии искусственных нейронных сетей и предметной области и онтологии действий по обработке искусственных нейронных сетей, которые являются основой развития обоих указанных направлений.
        \end{itemize}}
\end{scnrelfromvector}
\scniselement{раздел базы знаний}
\scnhaselementrole{ключевой sc-элемент}{Предметная область и онтология искусственных нейронных сетей}

\scnsegmentheader{Предметная область и онтология искусственных нейронных сетей}
\begin{scnsubstruct}

\scnheader{Предметная область искусственных нейронных сетей}
\scnidtf{Предметная область и.н.с.}
\scniselement{предметная область}

\begin{scnrelfromlist}{ключевой знак}
	\scnitem{Язык представления нейросетевого метода решения задач в базах знаний}
	\scnitem{Денотационная семантика Языка представления нейросетевого метода решения задач в базах знаний}
	\scnitem{Операционная семантика Языка представления нейросетевого метода в базах знаний}
\end{scnrelfromlist}

\begin{scnhaselementrolelist}{максимальный класс объектов исследования}
	\scnitem{искусственная нейронная сеть}
\end{scnhaselementrolelist}

\begin{scnhaselementrolelist}{класс объектов исследования}
    \scnitem{нейросетевой метод решения задач}
	\scnitem{формальный нейрон}
	\scnitem{навык решения задач с помощью искусственных нейронных сетей}
	\scnitem{синаптическая связь}
	\scnitem{слой и.н.с.}
	\scnitem{взвешенная сумма нейрона}
    \scnitem{искусственная нейронная сеть}
    \scnitem{искусственная нейронная сеть с прямыми связями}
    \scnitem{персептрон}
    \scnitem{персептрон Розенблатта}
    \scnitem{автоэнкодерная искусственная нейронная сеть}
    \scnitem{машина опорных векторов}
    \scnitem{искусственная нейронная сеть радиально-базисных функций}
    \scnitem{искусственная нейронная сеть с обратными связями}
    \scnitem{нейронная сеть Хопфилда}
    \scnitem{нейронная сеть Хэмминга}
    \scnitem{рекуррентная искусственная нейронная сеть}
    \scnitem{искусственная нейронная сеть Джордана}
    \scnitem{искусственная нейронная сеть Элмана}
    \scnitem{мультирекуррентная нейронная сеть}
    \scnitem{LSTM-элемент}
    \scnitem{GRU-элемент}
    \scnitem{полносвязная искусственная нейронная сеть}
    \scnitem{слабосвязная искусственная нейронная сеть}
    \scnitem{формальный нейрон}
    \scnitem{полносвязный формальный нейрон}
    \scnitem{сверточный формальный нейрон}
    \scnitem{рекуррентный формальный нейрон}
    \scnitem{синаптическая связь}
    \scnitem{параметр нейронной сети}
    \scnitem{настраиваемый параметр нейронной сети}
    \scnitem{весовой коэффициент}
    \scnitem{пороговое значение}
    \scnitem{ядро свертки}
    \scnitem{архитектурный параметр нейронной сети}
    \scnitem{количество слоев}
    \scnitem{количество формальных нейронов}
    \scnitem{количество синаптических связей}
    \scnitem{паттерн входной активности н.с.}
    \scnitem{признак}
    \scnitem{полносвязный слой и.н.с}
    \scnitem{сверточный слой и.н.с}
    \scnitem{слой и.н.с. нелинейного преобразования}
    \scnitem{dropout слой и.н.с.}
    \scnitem{pooling слой и.н.с}
\end{scnhaselementrolelist}
\begin{scnhaselementrolelist}{исследуемое отношение}
    \scnitem{формальный нейрон\scnrolesign}
    \scnitem{пороговый формальный нейрон\scnrolesign}
    \scnitem{синаптическая связь\scnrolesign}
    \scnitem{входное значение формального нейрона*}
    \scnitem{выходное значение формального нейрона*}
    \scnitem{функция активации*}
    \scnitem{взвешенная сумма*}
    \scnitem{распределяющий слой*}
    \scnitem{обрабатывающий слой*}
    \scnitem{выходной слой*}
\end{scnhaselementrolelist}

\begin{scnrelfromlist}{частная предметная область}
	\scnitem{Предметная область ИНС с заданным направлением связей}
	\begin{scnindent}
		\begin{scnrelfromlist}{частная предметная область}
			\scnitem{Предметная область ИНС с прямым связями}
			\begin{scnindent}
				\begin{scnrelfromlist}{частная предметная область}
					\scnitem{Предметная область персептронов}
					\begin{scnindent}
						\begin{scnrelfromlist}{частная предметная область}
							\scnitem{Предметная область персептронов Розенблатта}
							\scnitem{Предметная область персептронов Румельхарта}
							\scnitem{Предметная область автоэнкодерных ИНС}
						\end{scnrelfromlist}
					\end{scnindent}
					\scnitem{Предметная область ИНС радиально-базисных функций}
					\scnitem{Предметная область машин опорных векторов}
				\end{scnrelfromlist}
			\end{scnindent}
			\scnitem{Предметная область ИНС с обратными связями}
			\begin{scnindent}
				\scnidtf{Предметная область рекуррентных ИНС}
				\begin{scnrelfromlist}{частная предметная область}
					\scnitem{Предметная область ИНС Джордана}
					\scnitem{Предметная область ИНС Элмана}
					\scnitem{Предметная область LSTM-элементов}
					\scnitem{Предметная область GRU-элементов}
				\end{scnrelfromlist}
			\end{scnindent}
		\end{scnrelfromlist}
	\end{scnindent}
	\scnitem{Предметная область обучения ИНС}
	\begin{scnindent}
		\begin{scnrelfromlist}{частная предметная область}
			\scnitem{Предметная область ИНС, обучающихся с учителем}
			\scnitem{Предметная область ИНС, обучающихся без учителя}
			\begin{scnindent}
				\begin{scnrelfromlist}{частная предметная область}
					\scnitem{Предметная область обучающихся автоэнкодерных ИНС}
					\scnitem{Предметная область ИНС глубокого доверия}
					\scnitem{Предметная область генеративно-состязательных ИНС}
					\scnitem{Предметная область самоорганизующихся карт Кохонена}
					\scnitem{Предметная область ИНС Хопфилда}
					\scnitem{Предметная область подкрепляющего обучения ИНС}
				\end{scnrelfromlist}
			\end{scnindent}
		\end{scnrelfromlist}
	\end{scnindent}
	\scnitem{Предметная область топологий ИНC}
	\begin{scnindent}
		\begin{scnrelfromlist}{частная предметная область}
			\scnitem{Предметная область полносвязных ИНC}
			\scnitem{Предметная область многослойных ИНC}
			\scnitem{Предметная область слабосвязных ИНC}
		\end{scnrelfromlist}
	\end{scnindent}
	\scnitem{Предметная область задач, решаемых с помощью ИНС}
	\begin{scnindent}
		\begin{scnrelfromlist}{частная предметная область}
			\scnitem{Предметная область ИНС, решающих задачу классификации}
			\scnitem{Предметная область ИНС, решающих задачу аппроксимации}
			\scnitem{Предметная область ИНС, решающих задачу управления}
			\scnitem{Предметная область ИНС, решающих задачу фильтрации}
			\scnitem{Предметная область ИНС, решающих задачу детекции}
			\scnitem{Предметная область ИНС, решающих задачу с ассоциативной памятью}
		\end{scnrelfromlist}
	\end{scnindent}
	\scnitem{Предметная область интеграции ИНС с базой знаний}
\end{scnrelfromlist}

\scntext{введение}{Решатель задач занимается обработкой фрагментов базы знаний. На операционном уровне обработка сводится к добавлению, поиску, редактированию и удалению sc-узлов и sc-коннекторов базы знаний. На семантическом же уровне такая операция является действием, выполняемым в памяти субъекта действия, где, в общем случае, субъектом является ostis-система, а база знаний --- её памятью. действие определяется как процесс воздействия	одной сущности (или некоторого множества сущностей) на другую сущность (или на некоторое множество других сущностей) в соответствии с некоторой целью.}
\begin{scnindent}
	\begin{scnrelfromset}{смотрите}
		\scnitem{Предметная область и онтология действий, задач, планов, протоколов и методов, реализуемых ostis-системой, а также внутренних агентов, выполняющих эти действия}
	\end{scnrelfromset}
\end{scnindent}

\scnheader{искусственная нейронная сеть}
    \scnsubset{метод}
    \scntext{примечание}{Предлагается рассматривать и.н.с. как класс методов решения задач со своим языком представления.
    	\\Таким образом, искусственная нейронная сеть --- это нейросетевой метод решения задач. В соответствии с Технологией OSTIS,	спецификация класса методов решения задач сводится к спецификации соответствующего языка представления	методов, то есть к описанию его синтаксической, денотационной и операционной семантики.}
    \scntext{пояснение}{\textbf{\textit{искусственная нейронная сеть}} --- это биологически инспирированная математическая модель, обладающая обобщающей способностью после выполнения процедуры обучения. Под обобщающей способностью понимается способность модели выдавать корректные результаты для паттернов входной активности, не входящих в обучающую выборку.}
    \scnsubset{математическая модель}
    \begin{scnindent}
        \scntext{пояснение}{\textbf{\textit{математическая модель}} --- это упрощенное описание объекта реального мира, выраженное с помощью математической символики}
    \end{scnindent}
    \scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/neural_network_scg.png}}
    \begin{scnindent}
    	\scntext{примечание}{Пример формализации полносвязной двухслойной и.н.с. с двумя нейронами на входном слое и одном нейроне на обрабатывающем слоев.
    	    \\Следует отметить, что в практике авторов еще не было необходимости явно представлять и.н.с., как это показано	на рисунке. Чаще всего, представление и.н.с. сводилось к представлению ее операционной семантики в виде SCP-программы.}
    \end{scnindent}
    \scnrelfrom{решаемые задачи}{задачи, которые могут быть решены с помощью и.н.с. с приемлемой точностью}
        \begin{scnindent}
            \begin{scneqtoset}
                \scnitem{задача классификации}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scntext{пояснение}{Задача построения классификатора, т.е. отображения $\tilde c: X \rightarrow C$, где $ X \in \mathbb{R}\upperscore{m}$ ---
                    признаковое пространство п.в.а., $C \eq \scnleftcurlbrace C\underscore{1}, C\underscore{2}, ...,C\underscore{k} \scnrightcurlbrace$ --- конечное и обычно небольшое множество меток классов.}
                \end{scnindent}
                \scnitem{задача регрессии}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scntext{пояснение}{Задача построения оценочной функции по примерам $(x\underscore{i}, f(x\underscore{i}))$, где $f(x)$ --- неизвестная функция}
                    \scntext{пояснение}{\textbf{\textit{оценочная функция*}} --- отображение вида $\tilde{f}: X \rightarrow \mathbb{R}$, где $X \in \mathbb{R}\upperscore{m}$ --- признаковое пространство п.в.а.}
                \end{scnindent}
                \scnitem{задача кластеризации}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scntext{пояснение}{Задача разбиения множества п.в.а. на группы (кластеры) по какой-либо метрике сходства.}
                    \scntext{определение}{задача кластеризации --- это задача построения функции $a : X \to Y$ , которая любому объекту $x \in X$ ставит в соответствие номер кластера $y \in Y$ в соответствие с определенной метрикой расстояния $\rho(x, x' )$, где $X$ --- множество объектов, $Y$ --- множество номеров (имен, меток) кластеров, $x$, $x' \in X$.}
                \end{scnindent}
                \scnitem{задача понижения размерности}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scnidtf{задача уменьшения размерности признакового пространства}
                    \scntext{определение}{задача понижения размерности --- это задача построения функции $h : X \to Y$ , сохраняющей заданные соотношения между точками множеств $X$ и $Y$, где $X \subset \mathbb{R}\upperscore{p}$ , $Y \eq h(X) \subset \mathbb{R}\upperscore{q}$ , $q < p$.}
                \end{scnindent}
                \scnitem{задача управления}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scntext{определение}{задача управления --- это задача построения модели-регулятора состояния сложного динамического объекта.}
                \end{scnindent}
                \scnitem{задача фильтрации}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scntext{определение}{задача фильтрации --- это задача построения модели, которая производит очистку исходного сигнала, содержащего некоторый шум и уменьшает влияние случайных ошибок в сигнале.}
                \end{scnindent}
                \scnitem{задача детекции}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scnsubset{задача классификации}
                    \scnsubset{задача регрессии}
                    \scntext{определение}{задача детекции --- это задача, которая является частным случаем задачи классификации и задачи регрессии. Задача построения модели, осуществляющей обнаружение объектов определенных типов на фото- и видеоизображениях.}
                \end{scnindent}
                \scnitem{задача с ассоциативной памятью}
                \begin{scnindent}
                    \scnsubset{задача}
                    \scntext{определение}{задача с ассоциативной памятью --- это задача построения модели, позволяющей выполнить реконструкцию исходного образа на основании сохраненных ранее образов.}
                \end{scnindent}
            \end{scneqtoset}
        \end{scnindent}
\scntext{примечане}{Схожие задачи объединены в классы, для которых заданы обобщенные формулировки задач.}
\scntext{примечание}{Для достижения семантической совместимости с другими методами решения задач Технологии OSTIS, предлагается описывать нейросетевые методы внутри семантической памяти, соответственно, Синтаксис Языка представления нейросетевых методов решения задач в базах знаний является Синтаксисом SC-кода, использующимся в Технологии OSTIS для представления знаний.}
\scntext{примечание}{Таким образом, чтобы добавить в арсенал Технологии OSTIS нейросетевые методы решения задач и тем самым расширить круг задач, решаемых ostis-системами, необходимо описать денотационную и операционную семантику Языка представления нейросетевого метода решения задач в базах знаний.}

\scnheader{навык}
\scntext{примечание}{Понятие навыка описывает метод, интерпретация которого полностью может быть осуществлена данной кибернетической системой, в памяти которой хранится указанный метод. Таким образом, формируя спецификацию в	ostis-системе для нейросетевого метода решения задач и нейросетевой модели решения задач можно говорить о 	наличии у такой системы навыка решения задач с помощью и.н.с..}

\scnheader{SCg-текст. Фрагмент теоретико-множественной онтологии и.н.с}
\scnrelfrom{изображение}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/ontology_fragment.png}}
\begin{scnindent}
\scniselement{sc.g-текст}
\scntext{примечание}{Для примера, взяты класс задач классификации и конкретная обученная сверточная и.н.с.}
\scntext{примечание}{Фрагмент теоретико-множественной онтологии и.н.с., описывающий связь таких понятий и узлов, как:
	\begin{itemize}
		\item класс задач, решаемых с помощью и.н.с.
		\item класс нейросетевых методов решения задач
		\item нейросетевая модель решения задач
		\item навык решения задач с помощью и.н.с.
		\item конкретные задачи и методы их решения
		\end{itemize}}
\end{scnindent}

\scnheader{формальный нейрон\scnrolesign}
    \scnidtf{формальный нейронный элемент\scnrolesign}
    \scnidtf{нейронный элемент\scnrolesign}
    \scnidtf{нейрон\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{искусственная нейронная сеть}
    \scnrelfrom{второй домен}{формальный нейрон}
    \scnrelfrom{область определения}{искусственная нейронная сеть}
    \scntext{пояснение}{\textbf{\textit{формальный нейрон\scnrolesign}} --- ролевое отношение, связывающее искусственную нейронную сеть с ее нейроном.}

\scnheader{пороговый формальный нейрон\scnrolesign}
    \scnidtf{пороговый нейронный элемент\scnrolesign}
    \scnidtf{пороговый нейрон\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{искусственная нейронная сеть}
    \scnrelfrom{второй домен}{формальный нейрон}
    \scnrelfrom{область определения}{искусственная нейронная сеть}
    \scntext{пояснение}{\textbf{\textit{пороговый формальный нейрон\scnrolesign}} --- ролевое отношение, связывающее искусственную нейронную сеть с таким ее нейроном, выходное значение которого всегда равно -1.}
    \scntext{пояснение}{Весовой коэффициент синаптической связи, выходящей из такого нейрона, является порогом для нейрона, в который данная синаптическая связь входит.}

\scnheader{синаптическая связь\scnrolesign}
    \scnidtf{синапс\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{искусственная нейронная сеть}
    \scnrelfrom{второй домен}{синаптическая связь}
	\scnrelfrom{область определения}{\scnnonamednode}
	\begin{scnindent}
		\begin{scnreltoset}{объединение}
			\scnitem{искусственная нейронная сеть}
			\scnitem{синапс}
		\end{scnreltoset}
	\end{scnindent}
   \scndefinition{\textbf{\textit{синаптическая связь\scnrolesign}} --- ролевое отношение, связывающее искусственную нейронную сеть с ее синапсом.}

\scnheader{весовой коэффициент синаптической связи}
    \scnidtf{вес синапса}
    \scnidtf{сила синаптической связи}
    \scnsubset{настраиваемый параметр}
    \scntext{пояснение}{\textbf{\textit{весовой коэффициент синаптической связи}} --- это числовой коэффициент, который ставится в соответствие каждому синапсу нейронной сети и изменяется в процессе обучения.}
    \scntext{примечание}{Если сила синаптической связи отрицательна, то она называется \textit{тормозящей}. В противном случае она является \textit{усиливающей}.}

\scnheader{входное значение формального нейрона*}
    \scnidtf{входное значение нейрона*}
    \scnidtf{входное значение*}
    \scniselement{неролевое отношение}
    \scniselement{бинарное отношение}
    \scnrelfrom{первый домен}{формальный нейрон}
    \scnrelfrom{второй домен}{число}
    \scnrelfrom{область определения}{\scnnonamednode}
    \begin{scnindent}
    	\begin{scnreltoset}{объединение}
    		\scnitem{формальный нейрон}
    		\scnitem{число}
    	\end{scnreltoset}
    \end{scnindent}
    \scntext{пояснение}{\textbf{\textit{входное значение формального нейрона*}} --- неролевое отношение, связывающее нейрон входного слоя со значением признака п.в.а., который подается на вход нейронной сети.}
    \scntext{теоретическая неточность}{Использование множества как формы представления входных данных является серьезным допущением, так как на практике входные данные структурированы более сложно --- в многомерные массивы. Самым близким теоретическим аналогом здесь выступает тензор. К сожалению, описание теории нейронных сетей с помощью тензорного исчисления в литературе как таковое отсутствует, но активно используется на практике: например, во многих разрабатываемых нейросетевых фреймворках. Формализация нейронных сетей с помощью тензоров видится авторам наиболее вероятным направлением работы в ближайших изданиях \textit{стандарта OSTIS}.}

\scnheader{паттерн входной активности и.н.с.}
	\scnidtf{п.в.а.}
    \scniselement{мультимножество}
    \scniselement{кортеж}
    \scntext{пояснение}{\textbf{\textit{паттерн входной активности и.н.с.}} --- ориентированное мультимножество численных значений признаков некоторого объекта, которые могут выступать в качестве входных значений нейронов.}
	\scntext{примечание}{В текущей версии \textit{Стандарта OSTIS} предполагается, что п.в.а. содержит только предобработанные данные, то есть данные приведенные к численному виду и, возможно, преобразованные с помощью известных статистических методов (например, нормирования).}

\scnheader{признак}
    \scnidtf{feature}
    \scnidtf{множество признаков}
    \scnsubset{ролевое отношение}
    \scntext{пояснение}{\textbf{\textit{признак}} --- множество ролевых отношений, каждое из которых связывает некоторый п.в.а. с численным значением, которое характеризует данный п.в.а. с какой-либо стороны.}

\scnheader{взвешенная сумма*}
    \scnidtf{взвешенная сумма входных значений*}
    \scnidtf{в.с.}
    \scniselement{неролевое отношение}
    \scniselement{бинарное отношение}
    \scntext{пояснение}{\textbf{\textit{взвешенная сумма*}} --- неролевое отношение, связывающее формальный нейрон с числом, являющимся суммой произведений входных сигналов на весовые коэффициенты входящих в нейрон синаптических связей.}
     \scnrelfrom{область определения}{\scnnonamednode}
    \begin{scnindent}
    	\begin{scnreltoset}{объединение}
    		\scnitem{формальный нейрон}
    		\scnitem{число}
    	\end{scnreltoset}
    \end{scnindent}
    \scnrelfrom{первый домен}{формальный нейрон}
    \scnrelfrom{второй домен}{число}
    \scnrelfrom{формула}{
        \begin{equation*}
            S \eq \sum\underscore{i \eq 1}\upperscore{\scnleftcurlbrace n\scnrightcurlbrace} w\underscore{i} x\underscore{i} - T
        \end{equation*}}
    \begin{scnindent}
        \scntext{примечание}{\textit{n} --- размерность вектора входных значений, $w\underscore{i}$ --- \textit{i}-тый элемент вектора весовых коэффициентов, $x\underscore{i}$ --- \textit{i}-тый элемент вектора входных значений, \textit{T} --- пороговое значение.}
    \end{scnindent}

\scnheader{выходное значение формального нейрона*}
    \scnidtf{выходное значение нейрона*}
    \scnidtf{выходное значение*}
    \scniselement{неролевое отношение}
    \scniselement{бинарное отношение}
    \scnrelfrom{первый домен}{формальный нейрон}
    \scnrelfrom{второй домен}{число}
    \scnrelfrom{область определения}{\scnnonamednode}
    \begin{scnindent}
    	\begin{scnreltoset}{объединение}
    		\scnitem{формальный нейрон}
    		\scnitem{число}
    	\end{scnreltoset}
    \end{scnindent}
    \scntext{пояснение}{\textbf{\textit{входное значение*}} --- неролевое отношение, связывающее нейрон с числом, являющимся результатом применения функции активации нейрона к его взвешенной сумме.}
    \scntext{примечание}{Выходное значение нейрона является одним из входных сигналов для всех нейронов, в которые ведут выходящие из данного нейрона синапсы.}

\scnheader{слой и.н.с.}
    \scntext{примечание}{функция активации слоя является функцией активации всех формальных нейронов этого слоя}
    \scntext{примечание}{конфигурация слоя задается типом, количеством формальных нейронов, функцией активации}
    \scntext{примечание}{описание последовательности слоев и.н.с. с конфигурацией каждого слоя задает архитектуру и.н.с.}

\scnheader{распределяющий слой*}
    \scnidtf{входной слой*}
    \scniselement{неролевое отношение}
    \scniselement{бинарное отношение}
    \scndefinition{\textbf{\textit{распределяющий слой*}} --- неролевое отношение, связывающее искусственную нейронную сеть с ее слоем, нейроны которого принимают входные значения всей нейронной сети.}
    \scnrelfrom{область определения}{искусственная нейронная сеть}
    \scnrelfrom{первый домен}{искусственная нейронная сеть}
    \scnrelfrom{второй домен}{слой и.н.с.}

\scnheader{обрабатывающий слой*}
    \scniselement{неролевое отношение}
    \scniselement{бинарное отношение}
    \scndefinition{\textbf{\textit{обрабатывающий слой*}} --- неролевое отношение, связывающее искусственную нейронную сеть с ее слоем, нейроны которого принимают на вход выходные значения нейронов предыдущего слоя.}
    \scnrelfrom{область определения}{искусственная нейронная сеть}
    \scnrelfrom{первый домен}{искусственная нейронная сеть}
    \scnrelfrom{второй домен}{слой и.н.с}

\scnheader{выходной слой*}
    \scniselement{неролевое отношение}
    \scniselement{бинарное отношение}
    \scntext{пояснение}{\textbf{\textit{выходной слой*}} --- неролевое отношение, связывающее искусственную нейронную сеть с ее слоем, выходные значения нейронов которого являются выходными значениями всей нейронной сети.}
    \scnrelfrom{область определения}{искусственная нейронная сеть}
    \scnrelfrom{первый домен}{искусственная нейронная сеть}
    \scnrelfrom{второй домен}{слой и.н.с}

\bigskip
\end{scnsubstruct}
\scnendsegmentcomment{Предметная область и онтология искусственных нейронных сетей}

\scnsegmentheader{Предметная область и онтология действий по обработке искусственной нейронной сети}
\begin{scnsubstruct}

\scnheader{Предметная область действий по обработке искусственных нейронных сетей}
    \scnidtf{Предметная область действий по обработке и.н.с.}
    \scniselement{предметная область}
    \begin{scnhaselementrole}{максимальный класс объектов исследования}
        {действие по обработке искусственных нейронных сетей}
    \end{scnhaselementrole}
    \begin{scnhaselementrolelist}{класс объектов исследования}
        \scnitem{действие по обработке искусственных нейронных сетей}
        \scnitem{действие конфигурации весовых коэффициентов и.н.с.}
        \scnitem{действие конфигурации и.н.с.}
        \scnitem{действие интерпретации и.н.с.}
        \scnitem{метод обучения и.н.с.}
        \scnitem{метод обучения с учителем}
        \scnitem{метод обратного распространения ошибки}
        \scnitem{метод обучения без учителя}
        \scnitem{метод оптимизации}
        \scnitem{функция потерь}
        \scnitem{параметр обучения}
        \scnitem{скорость обучения}
        \scnitem{моментный параметр}
        \scnitem{параметр регуляризации}
        \scnitem{размер группы обучения}
        \scnitem{количество эпох обучения}
        \scnitem{выборка}
    \end{scnhaselementrolelist}
    \begin{scnhaselementrolelist}{исследуемое отношение}
        \scnitem{обучающая выборка\scnrolesign}
        \scnitem{тестовая выборка\scnrolesign}
        \scnitem{валидационная выборка\scnrolesign}
        \scnitem{метод обучения\scnrolesign}
        \scnitem{метод оптимизации\scnrolesign}
        \scnitem{функция потерь\scnrolesign}
    \end{scnhaselementrolelist}

\scnheader{действие по обработке искусственной нейронной сети}
    \scnidtf{действие по обработке и.н.с.}
    \scnidtf{действие с искусственной нейронной сетью}
    \scnsubset{действие}
    \scntext{пояснение}{В зависимости от того, является ли искусственная нейронная сеть знаком внешней по отношению к памяти системы сущности, элементы множества действие по обработке и.н.с. являются либо элементами множества \textbf{\textit{действие, выполняемое кибернетической системой в своей внешней среде}}, либо элементом множества \textbf{\textit{действие, выполняемое кибернетической системой в собственной памяти.}}.}
    \begin{scnsubdividing}
        \scnitem{действие конфигурации и.н.с.}
        \begin{scnindent}
        \begin{scnsubdividing}
            \scnitem{действие создания и.н.с.}
            \scnitem{действие редактирования и.н.с.}
            \scnitem{действие удаления и.н.с.}
            \scnitem{действие конфигурации слоя и.н.с.}
            \begin{scnindent}
                \begin{scnsubdividing}
                    \scnitem{действие добавления слоя в и.н.с.}
                    \scnitem{действие редактирования слоя и.н.с.}
                    \scnitem{действие удаления слоя и.н.с.}
                    \scnitem{действие установки функции активации нейронов слоя и.н.с.}
                    \scnitem{действие конфигурации нейрона в слое и.н.с.}
                    \begin{scnindent}
                        \begin{scnsubdividing}
                            \scnitem{действие добавления нейрона в слой и.н.с.}
                            \scnitem{действие редактирования нейрона в слое и.н.с.}
                            \scnitem{действие удаления нейрона из слоя и.н.с.}
                            \scnitem{действие установки функции активации нейрона в слое и.н.с.}
                        \end{scnsubdividing}
                    \end{scnindent}
                \end{scnsubdividing}
            \end{scnindent}
        \end{scnsubdividing}
        \end{scnindent}
        \scnitem{действие конфигурации весовых коэффициентов и.н.с.}
        \begin{scnindent}
            \scnsuperset{действие обучения и.н.с.}
            \scnsuperset{действие начальной инициализации весов и.н.с.}
            \begin{scnindent}
                \scnsuperset{действие начальной инициализации весов нейронов слоя и.н.с.}
                \begin{scnindent}
                    \scnsuperset{действие начальной инициализации весов нейрона и.н.с.}
                \end{scnindent}
            \end{scnindent}
        \end{scnindent}
        \scnitem{действие интерпретации и.н.с.}
    \end{scnsubdividing}
    \scntext{примечание}{Действия по обработке и.н.с осуществляет соответствующий коллектив агентов.}
    \scntext{пояснение}{Так как в результате действий по обработке и.н.с объект этих действий, конкретная и.н.с, может существенно меняться (меняется конфигурация сети, ее весовые коэффициенты), то и.н.с представляется в базе знаний как темпоральное объединение всех ее версий. Каждая версия является и.н.с. и темпоральной сущностью. На множестве этих темпоральных сущностей задается темпоральная последовательность с указанием первой и последней версии. Для каждой версии описываются специфичные знания. Общие для всех версий знания описываются для и.н.с, являющейся темпоральным объединением всех версий.}
    \begin{scnindent}
        \scnrelfrom{пример}{\scnfileimage[20em]{Contents/part_ps/src/images/sd_ps/sd_ann/temporal_neural_network_scg.png}}
    \end{scnindent}

\scnheader{действие обучения и.н.с.}
    \scnidtf{действие обучения искусственной нейронной сети}
    \scnsubset{действие конфигурации весовых коэффициентов и.н.с.}
    \scndefinition{\textbf{\textit{действие обучения и.н.с.}} --- действие, в ходе которого реализуется определенный метод обучения и.н.с. с заданными параметрами обучения и.н.с, методом оптимизации и функцией потерь.}
    \begin{scnrelfromset}{известные проблемы}
        \scnfileitem{Переобучение --- проблема, возникающая при обучении и.н.с., заключающаяся в том, что сеть хорошо адаптируется к п.в.а. из обучающей выборки, при этом теряя способность к обобщению. Переобучение возникает из-за применения неоправданно сложной модели при обучении и.н.с. Это происходит, когда количество настраиваемых параметров и.н.с. намного больше размера обучающей выборки. Возможные варианты решения проблемы заключаются в упрощении модели, увеличении выборки, использовании регуляризации (параметр регуляризации, техника dropout и т.д.).\\
            Обнаружение переобученности сложнее, чем недообученности. Как правило, для этого применяется кросс-валидация на валидационной выборке, позволяющая оценить момент завершения процесса обучения. Идеальным вариантом является достижение баланса между переобученностью и недообученностью.}
        \scnfileitem{Недообучение --- проблема, возникающая при обучении и.н.с., заключающаяся в том, что сеть дает одинаково плохие результаты на обучающей и контрольной выборках. Чаще всего такого рода проблема возникает при недостаточном времени, затраченном на обучение модели. Однако это может быть вызвано и слишком простой архитектурой модели либо малым размером обучающей выборки. Соответственно решение, которое может быть принято ML-инженером, заключается в устранении этих недостатков: увеличение времени обучения, использование модели с большим числом настраиваемых параметров, увеличение размера обучающей выборки, а также уменьшение регуляризации и более тщательный отбор признаков для обучающих примеров.}
    \end{scnrelfromset}
    \scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/ann_trainning_scg.png}}

\scnheader{выборка}
    \scnsubset{множество}
	\scntext{пояснение}{\textbf{\textit{выборка}} --- множество п.в.а., используемых в процессе обучения, тестирования и архитектурной настройки и.н.с.}

\scnheader{обучающая выборка\scnrolesign}
    \scnidtf{training set\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{действие обучения и.н.с.}
    \scnrelfrom{второй домен}{выборка}
    \scnrelfrom{область определения}{\scnnonamednode}
    \begin{scnindent}
    	\begin{scnreltoset}{объединение}
    		\scnitem{действие обучения и.н.с.}
    		\scnitem{выборка}
    	\end{scnreltoset}
    \end{scnindent}
    \scntext{пояснение}{\textbf{\textit{обучающая выборка\scnrolesign}} --- ролевое отношение, связывающее действие обучения и.н.с. с выборкой, используемой для изменения настраиваемых параметров и.н.с. в процессе ее обучения.}

\scnheader{тестовая выборка\scnrolesign}
    \scnidtf{test set\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{действие обучения и.н.с.}
    \scnrelfrom{второй домен}{выборка}
   	\scnrelfrom{область определения}{\scnnonamednode}
   	\begin{scnindent}
   		\begin{scnreltoset}{объединение}
   			\scnitem{действие обучения и.н.с.}
   			\scnitem{выборка}
   		\end{scnreltoset}
   	\end{scnindent}
    \scntext{пояснение}{\textbf{\textit{тестовая выборка\scnrolesign}} --- ролевое отношение, связывающее действие обучения и.н.с. с выборкой, используемой для проверки обобщающей способности обученной и.н.с.}
    \scntext{примечание}{Элементы контрольной выборки не используются в процессе обучения.}

\scnheader{валидационная выборка\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{действие обучения и.н.с.}
    \scnrelfrom{второй домен}{выборка}
    \scnrelfrom{область определения}{\scnnonamednode}
   	\begin{scnindent}
   		\begin{scnreltoset}{объединение}
   			\scnitem{действие обучения и.н.с.}
   			\scnitem{выборка}
		\end{scnreltoset}
   	\end{scnindent}
    \scntext{пояснение}{\textbf{\textit{валидационная выборка\scnrolesign}} --- ролевое отношение, связывающее действие обучения и.н.с. с выборкой, используемой для определения (настройки) архитектурных параметров и.н.с. и параметров обучения.}
    \scntext{примечание}{Элементы валидационной выборки не используются в процессе обучения (не входят в обучающую выборку).}

\scnheader{метод обучения и.н.с.}
    \scnsubset{метод}
	\scntext{пояснение}{\textbf{\textit{метод обучения и.н.с.}} --- метод итеративного поиска оптимальных значений настраиваемых параметров и.н.с., минимизирующих некоторую заданную функцию потерь.}
	\scntext{примечание}{Стоит отметить, что хотя целью применения метода обучения является минимизация функции потерь, \scnqqi{полезность} полученной после обучения модели можно оценить только по достигнутому уровню ее обобщающей способности.}
	\scnsuperset{метод обучения с учителем}
	\begin{scnindent}
		\scntext{пояснение}{\textbf{\textit{метод обучения с учителем}} --- метод обучения с использованием заданных целевых переменных.}
		\scnsuperset{метод обратного распространения ошибки}
		\begin{scnindent}
			\scnidtf{м.о.р.о.}
			\scntext{алгоритм}{\\
				\begin{minipage}{\linewidth}
					\begin{algorithm}[H]
						\KwData{$X$ --- данные, $Et$ --- желаемый отклик (метки), $E\underscore{m}$ --- желаемая ошибка (в соответствии с выбранной функцией потерь)}
						\KwResult{обученная нейронная сеть \textit{Net}}
						инициализация весов \textit{W} и порогов \textit{T};\\
						\Repeat{$E<E\underscore{m}$}{
							\ForEach{$x \in X$ $\And$ $e \in Et$}{
								фаза прямого распространения сигнала: вычисляются активации для всех слоев и.н.с.;\\
								фаза обратного распространения ошибки: вычисляются ошибки для последнего слоя и всех предшествующих слоев;\\
								изменение настраиваемых параметров и.н.с. в соответствии с вычисленными ошибками;\\
							}
							вычисление общей ошибки E на данной эпохе;
						}
					\end{algorithm}
				\end{minipage}}
			\scntext{примечание}{м.о.р.о. использует заданный метод оптимизации и заданную функцию потерь для реализации фазы обратного распространения ошибки и изменения настраиваемых параметров и.н.с. Одним из самых распространенных методов оптимизации является метод стохастического градиентного спуска. Приведенный м.о.р.о. используется для реализации последовательного варианта обучения.}
			\scntext{примечание}{Следует также отметить, что несмотря на то, что метод отнесен к методам обучения с учителем, в случае использования м.о.р.о. для обучения автокодировщиков в классических публикациях он рассматривается как метод обучения без учителя, поскольку в данном случае размеченные данные отсутствуют.}
		\end{scnindent}
	\end{scnindent}
	\scnsuperset{метод обучения без учителя}
	\begin{scnindent}
		\scntext{пояснение}{\textbf{\textit{метод обучения без учителя}} --- метод обучения без использования заданных целевых переменных (в режиме самоорганизации)}
		\scntext{пояснение}{В ходе выполнения алгоритма метода обучения без учителя выявляются полезные структурные свойства набора. Неформально его понимают как метод для извлечения информации из распределения, выборка для которого не была вручную аннотирована человеком.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Goodfellow2017}}
			\end{scnrelfromset}
		\end{scnindent}
	\end{scnindent}

\scnheader{метод обучения\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{действие обучения и.н.с.}
    \scnrelfrom{второй домен}{метод обучения и.н.с.}
    \scnrelfrom{область определения}{\scnnonamednode}
    \begin{scnindent}
    	\begin{scnreltoset}{объединение}
    		\scnitem{действие обучения и.н.с.}
    		\scnitem{метод обучения и.н.с.}
    	\end{scnreltoset}
    \end{scnindent}
    \scntext{пояснение}{\textbf{\textit{метод обучения\scnrolesign}} --- ролевое отношение, связывающее действие обучения и.н.с. с методом обучения,  использующимся для обучения и.н.с. в рамках этого действия.}

\scnheader{метод оптимизации}
    \scnsubset{метод}
	\scndefinition{\textbf{\textit{метод оптимизации}} --- метод для минимизации целевой функции потерь при обучении и.н.с.}
	\begin{scnrelfromlist}{включение}
		\scnitem{SGD}
			\begin{scnindent}
				\scnidtf{стохастический градиентный спуск}
				\scnidtf{с.г.с.}
				\scnidtf{stochastic gradient descent}
				\scntext{примечание}{В методе стохастического градиентного спуска корректировка настраиваемых параметров и.н.с. выполняется в направлении максимального уменьшения функции стоимости, т.е. в направлении, противоположном вектору градиента функции потерь.}
				\begin{scnindent}
					\begin{scnrelfromset}{источник}
						\scnitem{\scncite{Haykin2006}}
					\end{scnrelfromset}
				\end{scnindent}
			\end{scnindent}
		\scnitem{Nesterov method}
			\begin{scnindent}
				\scnidtf{метод Нестерова}
				\scntext{примечание}{Обучение методом с.г.с. иногда происходит очень медленно. Импульсный метод позволяет ускорить обучение, особенно в условиях высокой кривизны, небольших, но устойчивых градиентов или зашумленных градиентов. В импульсном методе вычисляется экспоненциально затухающее скользящее среднее прошлых градиентов и продолжается движение в этом направлении. Метод Нестерова является вариантом импульсного алгоритма, в котором градиент вычисляется после применения текущей скорости.}
				\begin{scnindent}
					\begin{scnrelfromset}{источник}
						\scnitem{\scncite{Goodfellow2017}}
					\end{scnrelfromset}
				\end{scnindent}
			\end{scnindent}
		\scnitem{AdaGrad}
			\begin{scnindent}
				\scnidtf{adaptive gradient}
				\scntext{примечание}{Данный метод по отдельности адаптирует скорости обучения всех настраиваемых параметров и.н.с., умножая их на коэффициент, обратно пропорциональный квадратному корню из суммы всех прошлых значений квадрата градиента.}
				\begin{scnindent}
					\begin{scnrelfromset}{источник}
						\scnitem{\scncite{Duchi2011}}
					\end{scnrelfromset}
				\end{scnindent}
			\end{scnindent}
		\scnitem{RMSProp}
			\begin{scnindent}
				\scnidtf{root mean square propagation}
				\scntext{примечание}{Данный метод является модификацией AdaGrad, которая позволяет улучшить его поведение в невыпуклом случае путем изменения способа агрегирования градиента на экспоненциально взвешенное скользящее среднее. Использование экспоненциально взвешенного скользящего среднего гарантирует повышение скорости сходимости после обнаружения выпуклой впадины, как если бы внутри этой впадины алгоритм AdaGrad был инициализирован заново.}
				\begin{scnindent}
					\begin{scnrelfromset}{источник}
						\scnitem{\scncite{Goodfellow2017}}
					\end{scnrelfromset}
				\end{scnindent}
			\end{scnindent}
			
		\scnitem{Adam}
			\begin{scnindent}
				\scnidtf{adaptive moments}
				\scntext{примечание}{Данный метод можно рассматривать как комбинацию RMSProp и AdaGrad. Помимо усредненного первого момента, данный метод использует усредненное значение вторых моментов градиентов}
				\begin{scnindent}
					\begin{scnrelfromset}{источник}
						\scnitem{\scncite{Kingma2014}}
					\end{scnrelfromset}
				\end{scnindent}
			\end{scnindent}
	\end{scnrelfromlist}
	\scntext{примечание}{Успешность применения методов оптимизации зависит главным образом от знакомства пользователя с соответствующим алгоритмом.}
	\begin{scnindent}
		\begin{scnrelfromset}{источник}
			\scnitem{\scncite{Goodfellow2017}}
		\end{scnrelfromset}
	\end{scnindent}

\scnheader{метод оптимизации\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{метод обучения и.н.с.}
    \scnrelfrom{второй домен}{метод оптимизации}
    \scnrelfrom{область определения}{\scnnonamednode}
    \begin{scnindent}
    	\begin{scnreltoset}{объединение}
    		\scnitem{метод обучения и.н.с.}
    		\scnitem{метод оптимизации}
    	\end{scnreltoset}
    \end{scnindent}
    \scntext{пояснение}{\textbf{\textit{метод оптимизации\scnrolesign}} --- ролевое отношение, связывающее метод обучения и.н.с. с методом оптимизации, использующимся для обучения и.н.с. с помощью данного метода.}

\scnheader{функция потерь}
    \scnsubset{функция}
	\scntext{пояснение}{\textbf{\textit{функция потерь}} --- функция, используемая для вычисления ошибки, рассчитываемой как разница между фактическим эталонным значением и прогнозируемым значением, получаемым и.н.с.}
    \begin{scnrelfromlist}{включение}
		\scnitem{MSE}
		\begin{scnindent}
			\scnidtf{mean square error}
			\scnidtf{средняя квадратичная ошибка}
			\scntext{формула}{
				\begin{equation*}
                    MSE \eq \frac{1}{L} \sum\underscore{l \eq 1}\upperscore{L} \sum\underscore{i \eq 1}\upperscore{m} (y\underscore{i}\upperscore{l} - e\underscore{i}\upperscore{l})\upperscore{2}
				\end{equation*}}
            \begin{scnindent}
                \scntext{примечание}{$y\underscore{i}\upperscore{l}$ --- прогноз модели, $e\underscore{i}\upperscore{l}$ --- ожидаемый (эталонный) результат, \textit{m} --- размерность выходного вектора, \textit{L} --- объем обучающей выборки.}
            \end{scnindent}
		\end{scnindent}
		\scnitem{BCE}
		\begin{scnindent}
			\scnidtf{binary cross entropy}
			\scnidtf{бинарная кросс-энтропия}
			\scntext{формула}{
				\begin{equation*}
                    BCE \eq - \sum\underscore{l \eq 1}\upperscore{L} (e\upperscore{l} \log(y\upperscore{l}) + (1 - e\upperscore{l})\log(1 - y\upperscore{l}))
				\end{equation*}}
            \begin{scnindent}
                \scntext{примечание}{$y\upperscore{l}$ --- прогноз модели, $e\upperscore{l}$ --- ожидаемый (эталонный) результат: \textit{0} или \textit{1}, \textit{L} --- объем обучающей выборки.}
            \end{scnindent}
			\scntext{примечание}{для бинарной кросс-энтропии в выходном слое и.н.с. будет находиться один нейрон}
		\end{scnindent}
		\scnitem{MCE}
		\begin{scnindent}
			\scnidtf{multi-class cross entropy}
			\scnidtf{мультиклассовая кросс-энтропия}
			\scntext{формула}{
				\begin{equation*}
                    MCE \eq - \sum\underscore{l \eq 1}\upperscore{L} \sum\underscore{i \eq 1}\upperscore{m} e\underscore{\scnleftcurlbrace i\scnrightcurlbrace\upperscore{l}} \log(y\underscore{\scnleftcurlbrace i\scnrightcurlbrace\upperscore{l}})
                \end{equation*}}
                \begin{scnindent}
                    \scntext{примечание}{$y\underscore{\scnleftcurlbrace i\scnrightcurlbrace\upperscore{l}}$ --- прогноз модели,  $e\underscore{i\upperscore{l}}$ --- ожидаемый (эталонный результат), \textit{m} --- размерность выходного вектора.}
                \end{scnindent}
			\scntext{примечание}{для мультиклассовой кросс-энтропии количество нейронов в выходном слое и.н.с. совпадает с количеством классов}
		\end{scnindent}
	\end{scnrelfromlist}
	\scntext{примечание}{Для решения задачи классификации рекомендуется использовать бинарную или мультиклассовую кросс-энтропийную функцию потерь, для решения задачи регрессии рекомендуется использовать среднюю квадратичную ошибку.}

\scnheader{функция потерь\scnrolesign}
    \scniselement{ролевое отношение}
    \scnrelfrom{первый домен}{метод обучения и.н.с.}
    \scnrelfrom{второй домен}{функция потерь}
    \scnrelfrom{область определения}{\scnnonamednode}
    \begin{scnindent}
    	\begin{scnreltoset}{объединение}
    		\scnitem{метод обучения и.н.с.}
    		\scnitem{функция потерь}
    	\end{scnreltoset}
    \end{scnindent}
    \scntext{пояснение}{\textbf{\textit{функция потерь\scnrolesign}} --- ролевое отношение, связывающее метод обучения и.н.с. с функцией потерь, использующимся для обучения и.н.с. с помощью данного метода.}

\scnheader{параметр обучения}
   \scnidtf{группа наиболее общих параметров метода обучения и.н.с.}
   \begin{scnrelfromset}{состав группы параметров обучения}
       \scnitem{скорость обучения}
          \begin{scnindent}
              \scntext{пояснение}{\textbf{\textit{скорость обучения}} --- параметр, определяющий скорость изменения параметров и.н.с. в процессе обучения.}
          \end{scnindent}
       \scnitem{моментный параметр}
          \begin{scnindent}
              \scnidtf{момент}
              \scnidtf{momentum}
              \scntext{пояснение}{\textbf{\textit{моментный параметр}} --- параметр, используемый в процессе обучения для устранения проблемы \scnqqi{застревания} алгоритма обучения в локальных минимумах минимизируемой функции потерь.}
              \scntext{пояснение}{При обучении и.н.с. частой является ситуация остановки процесса в определенной точке локального минимума без достижения желаемого уровня обобщающей способности и.н.с. Для устранения такого нежелательного явления вводится дополнительный параметр (момент) позволяющий алгоритму обучения \scnqqi{перескочить} через локальный минимум и продолжить процесс.}
         \end{scnindent}
       \scnitem{параметр регуляризации}
         \begin{scnindent}
             \scntext{пояснение}{\textbf{\textit{параметр регуляризации}} --- параметр, применяемый для контроля уровня переобучения и.н.с.}
             \scntext{пояснение}{\textbf{\textit{регуляризация}} --- добавление дополнительных ограничений к правилам изменения настраиваемых параметров и.н.с. с целью предотвратить переобучение.}
         \end{scnindent}
       \scnitem{размер группы обучения}
         \begin{scnindent}
             \scntext{пояснение}{\textbf{\textit{размер группы обучения}} --- размер группы п.в.а., которая используется для изменения параметров и.н.с. на каждом элементарном шаге обучения.}
         \end{scnindent}
       \scnitem{количество эпох обучения}
       \begin{scnindent}
       		\scntext{пояснение}{\textbf{\textit{эпоха обучения}} --- одна итерация алгоритма обучения, в ходе которой все обучающие п.в.а. из обучающей выборки были однократно использованы.}
       \end{scnindent}
   \end{scnrelfromset}

\bigskip
\end{scnsubstruct}
\scnendsegmentcomment{Предметная область и онтология действий по обработке искусственной нейронной сети}

\bigskip
\end{scnsubstruct}
\scnendcurrentsectioncomment

\end{SCn}
