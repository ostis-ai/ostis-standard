\begin{SCn}
\scnsectionheader{Предметная область и онтология операционной семантики Языка представления нейросетевого метода решения задач}
\begin{scnsubstruct}
\scniselement{раздел базы знаний}
\scnhaselementrole{ключевой sc-элемент}{Операционная семантика Языка представления нейросетевого метода решения задач}
		
\scnheader{Операционная семантика Языка представления нейросетевого метода решения задач}
\scntext{примечание}{Операционной семантикой любого языка представления методов решения задач является спецификация семейства агентов, обеспечивающих интерпретацию любого метода, принадлежащего соответствующему классу методов. Это семейство является интерпретатором соответствующего метода решения задач. В рамках технологии OSTIS такой интерпретатор называется моделью решения задач. Так как в рамках Технологии OSTIS используется многоагентный подход, то разработка нейросетевой модели решения задач сводится к разработке агентно-ориентированной	модели интерпретации и.н.с.}
\scntext{примечание}{\textbf{\textit{Операционная семантика Языка представления нейросетевого метода в базах знаний}} задается \textit{многоагентный подход} к интерпретации \textit{искусственных нейронных сетей} и спецификацией соответствующих действий.}
\scntext{примечание}{Нейросетевой метод описан в виде программы на некотором \textit{языке программирования}, который может быть как внешним по отношению к \textit{ostis-системе}, так и внутренним (на данный момент, \textit{Язык SCP}). Каждому такому \textit{языку программирования} соответствует некоторая дочерняя \textit{предметная область} \textit{Предметная область нейросетевых методов}}
\begin{scnindent}
	\scnrelfrom{источник}{\scncite{Kovalev2022}}
\end{scnindent}
\scntext{примечание}{В случае описания \textit{нейросетевого метода} на внешнем языке, такой метод описывается в соответствующей предметной области, в рамках которой также специфицируется действие интерпретации данного метода. Данному действию соответствует агент, реализованный на соответствующем \textit{языке программирования}.
	\\Однако для достижения конвергенции и интеграции необходимо описывать нейросетевые методы на внутреннем языке ostis-системы, которым является \textit{Язык SCP}.
	\\Интерпретация \textit{scp-программы} сводится к агентно-ориентированной обработке действий в sc-памяти. Этими действиями являются \textit{scp-операторы}.}

\scnheader{Предметная область нейросетевых методов}
\scnidtf{Предметная область искусственных нейронных сетей}
\begin{scnrelfromlist}{дочерняя предметная область}
    \scnitem{Предметная область нейросетевых методов SCP}
    \scnitem{Предметная область нейросетевых методов Python}
    \scnitem{Предметная область нейросетевых методов C++}
\end{scnrelfromlist}

\scnheader{действие интерпретации слоя и.н.с.}
\begin{scnrelfromset}{декомпозиция}
	\scnitem{действие вычисления взвешенной суммы всех нейронов слоя}
	\scnitem{действие вычисления функции активации всех нейронов слоя}
	\scnitem{действие интерпретации сверточного слоя}
	\scnitem{действие интерпретации пулинг слоя}
\end{scnrelfromset}
\scntext{примечание}{При необходимости задавать различные аргументы для нейронов одного и того же слоя, можно специфицировать соответствующие действия, однако на данный момент этого не было произведено из-за слабой изученности подобного рода \textit{нейросетевых моделей решения задач}.}

\scnheader{ориентированное множество чисел}
\scnidtf{ормножество чисел}
\scnrelto{включение}{число}
\scnrelto{включение}{ориентированное множество}
\scnrelto{первый домен}{строковое представление ормножества чисел*}
\scntext{примечание}{Для описания спецификации указанных действий необходимо ввести понятия \textit{ориентированного множества чисел} и \textit{матрицы}, с помощью которых задаются входные значения \textit{и.н.с.}, выходные значения \textit{и.н.с.}, матрицы весовых коэффициентов и прочее.
	\\Каждый элемент ориентированного множества чисел является некоторым числом. Числа могут быть представлены в виде sc-узлов, либо с помощью строкового представления всего множества, для чего используется специальное отношение \textit{строковое представление ормножества чисел*}, которое введено в целях оптимизации некоторых вариантов реализации агента, интерпретирующего действие, использующее понятие ориентированного множества чисел.}
	
\scnheader{матрица}
\scntext{примечание}{\textit{матрица} является \textit{ориентированным множеством} \textit{ориентированных множеств} чисел равной мощности.}

\scnheader{Действие вычисления взвешенной суммы всех нейронов слоя}
\scntext{примечание}{Аргументы (\textit{объекты\scnrolesign}) этого действия задаются следующими отношениями: \textit{входной вектор\scnrolesign}, \textit{матрица весовых коэффициентов нейронов слоя\scnrolesign}.}
\scnhaselementrole{результат}{ориентированное множество чисел, являющихся взвешенной суммой нейронов соответствующего слоя.}

\scnheader{входной вектор\scnrolesign}
\scnrelfrom{первый домен}{действие интерпретации и.н.с.}
\scnrelfrom{второй домен}{ориентированное множество чисел}

\scnheader{матрица весовых коэффициентов нейронов слоя\scnrolesign}
\scnrelfrom{первый домен}{действие по обработке и.н.с.}
\scnrelfrom{второй домен}{матрица}

\scnheader{SCg-текст. Пример действия вычисления взвешенной суммы всех нейронов слоя}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/action_weighted_sum.png}}
\begin{scnindent}
	\scntext{примечание}{Пример спецификации действия вычисления взвешенной суммы всех нейронов слоя для слоя с двумя нейронами и входным вектором размерностью 2}
\end{scnindent}

\scnheader{Действие вычисления функции активации всех нейронов слоя}
\scntext{примечание}{Аргументы этого действия задаются следующими отношениями: \textit{вектор взвешенных сумм нейронов слоя\scnrolesign}, \textit{вектор порогов нейронов слоя\scnrolesign}, \textit{функция активации\scnrolesign}.}
\scnhaselementrole{результат}{ориентированное множество чисел, являющихся выходными значениями нейронов слоя}

\scnheader{вектор взвешенных сумм нейронов слоя\scnrolesign}
\scnrelfrom{первый домен}{действие по обработке и.н.с.}
\scnrelfrom{второй домен}{ориентированное множество чисел}

\scnheader{вектор порогов нейронов слоя\scnrolesign}
\scnrelfrom{первый домен}{действие по обработке и.н.с.}
\scnrelfrom{второй домен}{ориентированное множество чисел}

\scnheader{функция активации\scnrolesign}
\scnrelfrom{первый домен}{действие по обработке и.н.с.}
\scnrelfrom{второй домен}{функция}
\scntext{примечание}{Любой агент, интерпретирующий действия с заданными с помощью отношения \textit{функция активации\scnrolesign} аргументами, должен использовать интерпретатор математических функций. использующихся в качестве функций активации.}

\scnheader{Действие интерпретации сверточного слоя}
\scntext{примечание}{Аргументы этого действия задаются следующими отношениями: \textit{входная матрица\scnrolesign}, \textit{ядро свертки\scnrolesign}, \textit{шаг свертки\scnrolesign}.}
\scnhaselementrole{результат}{Результатом действия является матрица, полученная в результате свертки входной матрицы с ядром свертки.}

\scnheader{входная матрица\scnrolesign}
\scnrelfrom{первый домен}{действие интерпретации и.н.с.}
\scnrelfrom{второй домен}{матрица}

\scnheader{ядро свертки\scnrolesign}
\scnrelfrom{первый домен}{действие интерпретации сверточного слоя}
\scnrelfrom{второй домен}{матрица}

\scnheader{шаг свертки\scnrolesign}
\scnrelfrom{первый домен}{действие интерпретации сверточного слоя}
\scnrelfrom{второй домен}{число}

\scnheader{Действие интерпретации пулинг слоя}
\scntext{примечание}{Аргументы этого действия задаются следующими отношениями: \textit{шаг окна пулинга\scnrolesign}, \textit{размер окна пулинга\scnrolesign}, \textit{входная матрица\scnrolesign}}
\scnhaselementrole{результат}{матрица, полученная в результате пулинга входной матрицы.}

\scnheader{входная матрица\scnrolesign}
\scnrelfrom{первый домен}{действие интерпретации и.н.с.}
\scnrelfrom{второй домен}{матрица}

\scnheader{размер окна пулинга\scnrolesign}
\scnrelfrom{первый домен}{действие интерпретации пулинг слоя}
\scnrelfrom{второй домен}{матрица}

\scnheader{шаг окна пулинга\scnrolesign}
\scnrelfrom{первый домен}{действие интерпретации пулинг слоя}
\scnrelfrom{второй домен}{число}

\scnheader{интерпретатор искусственных нейронных сетей}
\scntext{примечание}{Спецификация агентов, соответствующих указанным действиям, задает агентно-ориентированную модель интерпретации искусственных нейронных сетей. Реализация этой модели будет называться интерпретатором искусственных нейронных сетей}
\scntext{примечание}{Реализация интерпретатора описанных в данной главе действий по построению \textit{и.н.с.} и описания в базе знаний экспертных знаний разработчиков\textit{и.н.с.} (а значит реализация интеллектуальной среды проектирования \textit{и.н.с.}) позволит автоматически, исходя из описания задачи, генерировать нейросетевые методы в памяти \textit{ostis-системы}, что является одним из ключевых направлений дальнейшего развития конвергенции и интеграции и.н.с. с базами знаний.}

\scnheader{Рисунок. Решение задачи \scnqq{ИСКЛЮЧАЮЩЕЕ ИЛИ}}
\scntext{примечание}{Рассмотрим пример описания \textit{нейросетевого метода}, решающего задачу, которая формулируется следующим образом: вычислить результат логической операции \scnqq{ИСКЛЮЧАЮЩЕЕ ИЛИ} для значений двух логических переменных. На рисунке представлено решение этой задачи с помощью сигнальной функции.}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/strong_or_graphic.png}}

\scnheader{однослойный персептрон}
\scntext{примечание}{В работе описан однослойный персептрон, решающий поставленную задачу. Персептрон состоит из двух входных нейронов и одного выходного, с заданным порогом в 0,5 и сигнальной функцией активации:
	\begin{equation*}
		F(S) =
	 	\begin{cases}
	 		1, 0 < S < 0,\\
	 		0, else
	 	\end{cases}
	 \end{equation*}}
\begin{scnindent}
	\begin{scnrelfromset}{источник}
		\scnitem{\scncite{Golovko2017}}
	\end{scnrelfromset}
\end{scnindent}

\scnheader{Рисунок. Схема однослойного персептрона, решающего задачу \scnqq{ИСКЛЮЧАЮЩЕЕ ИЛИ}}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/strong_or_ann.png}}

\scnheader{Рисунок. Метод, решающий задачу \scnqq{ИСКЛЮЧАЮЩЕЕ ИЛИ}, представленный с помощью языка представления нейросетевых методов SCP}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/exclusive_or_ann_scp.png}}

\scnheader{SCg-текст. Представление сигнальной функции активации в памяти ostis-системы}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/signal_function_def.png}}
\scntext{примечание}{Весовые коэффициенты синапсов входного слоя равны 1. На рисунке \textit{Рисунок. Схема однослойного персептрона, решающего задачу \scnqq{ИСКЛЮЧАЮЩЕЕ ИЛИ}} представлена схема персептрона.
	\\Данному персептрону соответствует метод, представленный в базе знаний ostis-системы на описанном в этой главе языке представления нейросетевых методов SCP. Данный метод представлен на рисунке \textit{Рисунок. Метод, решающий задачу \scnqq{ИСКЛЮЧАЮЩЕЕ ИЛИ}, представленный с помощью языка представления нейросетевых методов SCP}.
	\\Описание метода состоит из последовательности двух обобщенных спецификаций действий --- действия вычисления взвешенной суммы всех нейронов слоя и действия вычисления функции активации для всех нейронов слоя.
	\\Сигнальная функция активации, использующаяся в персептроне, в памяти ostis-системы определяется логической формулой, представленной на рисунке \textit{SCg-текст. Представление сигнальной функции активации в памяти ostis-системы}.}



\scnsectionheader{Логико-семантическая модель ostis-системы автоматизации проектирования искусственных нейронных сетей, семантически совместимых с базами знаний ostis-систем}
\begin{scnsubstruct}

\scnheader{Логико-семантическая модель ostis-системы автоматизации проектирования искусственных нейронных сетей}
\begin{scnhaselementrolelist}{класс объектов исследования}
	\scnitem{действие трансляции условия задачи}
	\scnitem{действие классификации задачи}
	\scnitem{действие поиска подходящей обучающей выборки}
	\scnitem{действие формирования требований к обучающей выборке}
	\scnitem{действие очистки выборки}
	\scnitem{действие выявления содержательных признаков}
	\scnitem{действие трансформации выборки}
	\scnitem{действие разбиения выборки}
	\scnitem{действие выбора класса нейросетевых методов}
	\scnitem{действие формирования спецификации входов и выходов и.н.с.}
	\scnitem{действие выбора метода оптимизации}
	\scnitem{действие выбора минимизируемой функции ошибки}
	\scnitem{действие начальной инициализации и.н.с.}
	\scnitem{действие выбора гиперпараметров и.н.с.}
	\scnitem{метод обучения с учителем}
	\scnitem{метод обучения без учителя}
	\scnitem{действие обучения и.н.с.}
\end{scnhaselementrolelist}

\scnheader{Язык представления нейросетевых методов в базах знаний}
\scniselement{язык представления методов}
\scntext{примечание}{Наличия \textit{Языка представления нейросетевых методов в базах знаний} и его интерпретатора позволяет обеспечить интерпретацию \textit{нейросетевого метода} в памяти \textit{ostis-системы}. Наличие в единой памяти не только экземпляров методов, но и понятий, их описывающих, создает основу для автоматизации процесса построения нейросетевых методов. 
	\\В памяти \textit{ostis-системы} хранятся знания о том, методы какого класса могут решить задачу заданного класса, но экземпляров класса этого метода может не быть представлено в системе. На этот случай система должна иметь возможность сообщить пользователю о возможности решения, для которого, однако, необходимо погрузить в систему определенный метод. Так как система хранит в единой памяти задачу и требования к методу ее решения, появляется возможность спроектировать необходимый метод. Для этого необходимо наличие среды проектирования методов соответствующих классов. В случае \textit{нейросетевого метода}, речь идет об интеллектуальной среде построения \textit{нейросетевых методов}.}

\scnheader{нейросетевой метод}
\scntext{примечание}{В основе интеллектуальной среды построения \textit{нейросетевых методов} лежат соответствующие другу другу иерархии действий, задач и методов построения \textit{и.н.с.} Наличие такой иерархии позволит описать язык представления методов построения \textit{и.н.с.} и разработать интерпретатор этого языка.}
\scntext{примечание}{Построение иерархии соответствующих действий построения \textit{и.н.с.} следует начать с изучения этапов проектирования и обучения \textit{и.н.с.}, которые, в общем случае, выполняют все разработчики и.н.с.:
	\\1. Постановка задачи\\
	2. Предобработка выборки: очистка\\
	3. Предобработка выборки: выявление содержательных признаков\\
	4. Предобработка выборки: трансформация\\
	5. Разбиение выборки на обучающую, валидационную и тестовую (контрольную)\\
	6. Выбор класса нейросетевых методов в соответствии со сформулированной задачей\\
	7. Формирование спецификации на входные и выходные данные\\
	8. Выбор метода оптимизации\\
	9. Выбор минимизируемой функции ошибки\\
	10. Начальная инициализация параметров нейронной сети\\
	11. Выбора гиперпараметров и.н.с.\\
	12. Обучение модели на обучающей выборке\\
	13. Оценка эффективности и.н.с}

\scnheader{Постановка задачи}
\scntext{примечание}{Постановка задачи включает в себя описание входных данных (изображения/видео, временные ряды, текст), выходных данных и требований к методу решения (скорость, затраты по памяти и так далее). Также описывается дополнительная информация, которая может помочь в построении метода решения задачи (к примеру, спецификация обучающей выборки, если таковая имеется). Обычно, на данном этапе разработчик и.н.с. определяет класс задачи, формирует требования к обучающей выборке, если она не предоставлена.
	\\Выполнение данного этапа средой проектирования \textit{и.н.с.} подразумевает выполнение следующих действий:
	\begin{itemize}
		\item \textbf{\textit{действие трансляции условия задачи}}. Действие транслирует заданное с помощью \textit{интерфейса ostis-системы} (к примеру, естественно-языкового интерфейса) описание задачи в память ostis-системы. Действие необходимо в случае, когда условие задачи задается пользователем. Необходимо понимать, что описание задачи поступает в базу знаний не только от \textit{пользовательского интерфейса}. К примеру, задача может быть сформулирована самой системой в ходе ее жизнедеятельности.
		Данное действие является общим для всех ostis-систем, поэтому его рассмотрение выходит за рамки рассмотрения процесса построения интеллектуальной среды проектирования \textit{и.н.с.}
		\item \textbf{\textit{действие классификации задачи}}. Действие определяет класс задачи (задача регрессии, детекции, кластеризации и так далее), исходя из описания задачи в базе знаний.
		\item \textbf{\textit{действие поиска подходящей обучающей выборки}}. В базе знаний может храниться набор спецификаций выборок, к которым у ostis-системы есть доступ. Действие производит поиск выборок, которые могут быть использованы в качестве обучающей выборки.
		\item \textbf{\textit{действие формирования требований к обучающей выборке}}. Если обучающая выборка не была предоставлена и не была найдена, то необходимо сформировать описание требований к обучающей выборке, которое можно будет транслировать на язык пользовательского интерфейса и запросить необходимую выборку у пользователя.
	\end{itemize}}

\scnheader{Предобработка выборки}
\scnhaselement{очистка}
\begin{scnindent}
	\scntext{примечание}{На этом этапе обнаруживаются признаки, которые имеют в общем случае некорректные значения (например, для каких-то образов значение признака может иметь неопределенное значение, либо значение, не совпадающее по типу, либо аномально большое или очень маленькое значение, которое встречается в редком числе случаев). Для признаков, имеющих неопределенное значение, может быть применены различные методы устранения, например, такие значения могут быть заменены средним значением этого признака, рассчитанным по всем образам (для непоследовательных данных), либо они могут быть заменены средним значением по соседним образам (в случае временных рядов), либо каким-то фиксированным значением. Радикальная мера решения проблемы --- удаление образов, имеющих неопределенные значения признаков из выборки. Однако его лучше применять, если образов с отсутствующими значениями признаков немного. Для выбросов и аномалий применяются схожие стратегии (но только в том случае, если задача не состоит в прогнозировании этих аномалий).
		\\В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия очистки выборки}}, которое выполняется в случае обработки выборки, которая ранее не была представлена в памяти системы (к примеру, была получена от пользователя).
		\\Реализация интерпретатора (агента) данного действия требует описания в памяти классификации стратегий очистки данных и реализации методов применения этих стратегий.}
\end{scnindent}
\scnhaselement{выявление содержательных признаков}
\begin{scnindent}
	\scntext{примечание}{Осуществляется инжиниринг признаков, состоящий в отборе признаков, влияющих на результат работы модели, несодержательные признаки, которые никак не коррелируют с выходом модели, удаляются. Цель этого этапа --- уменьшение размерности пространства признаков для снижения влияния эффекта переобучения на модель.
		\\Для снижения размерности признакового пространства может применяться методы отбора признаков и выделения признаков.
		\\При отборе признаков, осуществляется формирование подмножества из исходных признаков (алгоритм последовательного обратного отбора, рекурсивный алгоритм обратного устранения признаков,  алгоритмы с использованием случайных лесов).
		\\При выделении признаков из набора признаков извлекается информация для построения нового подпространства признаков (алгоритмы с использованием автоэнкодера).
		\\В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия выявления содержательных признаков}}. Реализация интерпретатора (агента) данного действия требует описания в памяти классификации стратегий уменьшения размерности признакового пространства и реализации методов применения этих стратегий.}
\end{scnindent}
\scnhaselement{трансформация}
\begin{scnindent}
	\begin{scnrelfromvector}{примечание}
		\scnfileitem{На этом этапе осуществляется подготовка данных к обучению.}
		\scnfileitem{Здесь следует уделить особое внимание наличию категориальных признаков, чаще всего заданных строковыми типами. Эти признаки могут быть номинальными и порядковыми. Для кодирования порядковых признаков чаще всего применяют последовательный числовой код (1, 2, 3,...). Для кодирования номинальных такое решение неверно, так как эти признаки равноправны и не могут сравниваться по числовому коду (например, пол --- 0/1). Для номинальных признаков применяется способ прямого кодирования, заключающийся в создании и использовании фиктивных признаков по количеству значений исходного. Например, признак пол (мужской, женский) преобразуется в два новых признака мужской и женский с соответствующими значениями для имеющихся образов.}
		\scnfileitem{Масштабирование признаков предполагает приведение значений признаков к одному общему интервалу --- это особенно актуально для признаков, имеющих несоразмерные выборочные средние значения по всем образам --- например, один признак в среднем имеет значение 10.000, а другой 12. Это может проявится в выполнении минимизации только по признаку с наибольшими значениями и плохой сходимости метода обучения. Чаще всего масштабирование соответствует выполнению нормализации на отрезок (min-max нормализация)}
		\begin{scnindent}
			\scnrelfrom{формула}{
				\begin{equation*}
					x\underscore{norm}\upperscore{i} = \frac{x\upperscore{i} - x\underscore{min}}{x\underscore{max} - x\underscore{min}}
				\end{equation*}}
			\begin{scnindent}
				\scntext{примечание}{$x\upperscore{i}$ --- значение признака для отдельно взятого образа \textit{i}, $x\underscore{min}$ --- наименьшее значение для признака, $x\underscore{max}$ --- наибольшее значение для признака.}
			\end{scnindent}
		\end{scnindent}
		\scnfileitem{Другой вариант масштабирования --- применение стандартизации признаков}
		\begin{scnindent}
			\scnrelfrom{формула}{
				\begin{equation*}
					x\underscore{std}\upperscore{i} = \frac{x\upperscore{i} - \mu(x)}{\sigma(x)}
				\end{equation*}}
			\begin{scnindent}
				\scntext{примечание}{$\mu(x)$ --- выборочное среднее отдельного признака, $\sigma(x)$ --- стандартное отклонение.}
			\end{scnindent}
		\end{scnindent}
		\scnfileitem{Стандартизация сохраняет полезную информацию о выбросах в исходных данных и делает алгоритм обучения менее чувствительным к ним.}
		\scnfileitem{Дискретизация применяется для перехода от вещественного признака к порядковому за счет кодирования интервалов одним значением (например, если признак отражает возраст человека, то может быть произведена дискретизация значений с выделением определенных возрастных групп, где каждая группа будет кодироваться одним целым числом).}
		\scnfileitem{В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия трансформации выборки}}. Реализация интерпретатора (агента) данного действия требует описания в памяти классификации методов масштабирования признаков и реализации методов применения этих стратегий.}
	\end{scnrelfromvector}
\end{scnindent}

\scnheader{Разбиение выборки на обучающую, валидационную и тестовую (контрольную)}
\scntext{примечание}{Производится разбиение всей выборки данных, на обучающую, тестовую и, в некоторых случаях, валидационную.
	\\Валидационная выборка используется для оценки влияния изменения гиперпараметров на результат обучения и может применяться как дополнительный инструмент для этого наравне с сеточным поиском.
	\\Разбиение проводится в соотношении 3:1:1, в процентах (60/20/20), если валидационная выборка не используется, то 80/20.
	\\В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия разбиения выборки}}.
	\\Все предыдущие этапы применялись к выборке, последующие этапы относятся к используемым моделям и.н.с.}

\scnheader{Выбор класса нейросетевых методов в соответствии со сформулированной задачей}
\scntext{примечание}{На этом этапе осуществляется выбор основной архитектуры и.н.с., которая будет использоваться при обучении. Однако, нужно отметить, что этот выбор относительно условный, то есть исследователь не ограничен использованием только одного типа и.н.с. для решения задачи (как, например, сверточной сети для изображений, поскольку изображения можно обрабатывать и обычным многослойным персептроном). Речь скорее идет именно о рекомендованной архитектуре, но это не исключает использование любых других вариантов архитектур и их сочетаний в рамках одной модели).
	\\Примерами таких рекомендаций являются:
	\begin{itemize}
		\item изображения/видео --- сверхточные нейронные сети;
		\item временные ряды --- многослойные персептроны или рекуррентные сети;
		\item текстовая информация --- многослойные персептроны или рекуррентные сети;
		\item наборы характеристик некоторых объектов (например, спецификации автомобилей) --- многослойный персептрон.
	\end{itemize}
	В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия выбора класса нейросетевых методов}}.}

\scnheader{Формирование спецификации на входные и выходные данные}
\scntext{примечание}{Выполняются дополнительные преобразования данных, связанные с изменением структур хранения (например, преобразование многомерного массива в одномерный, конвертация типов)
	\\В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия формирования спецификации входов и выходов и.н.с.}}.}

\scnheader{Выбор метода оптимизации}
\scntext{примечание}{В рамках ПрО и.н.с. описаны следующие методы оптимизации:
	\begin{itemize}
		\item стохастический градиентный спуск (stochastic gradient descent --- SGD);
		\item метод Нестерова;
		\item адаптивный градиент (adaptive gradient --- AdaGrad);
		\item адаптивная оценка момента (adaptive moment estimation --- Adam);
		\item среднеквадратическое распространение (root mean square propagation --- RMSProp).
	\end{itemize}
	В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия выбора метода оптимизации}}.}

\scnheader{Выбор минимизируемой функции ошибки}
\begin{scnrelfromvector}{примечание}
	\scnfileitem{На этом этапе задается функция ошибок, которая будет минимизироваться. К примеру, MSE лучше подходит для задач регрессии и для кластеризации, CE --- для классификационных задач. Далее приведены примеры.}
	\scnfileitem{
		\begin{equation*}
			MSE = \frac{1}{n} \sum\underscore{i=1}\upperscore{n} (Y\underscore{i} - \widetilde{Y\underscore{i}})\upperscore{2}
		\end{equation*}}
		\begin{scnindent}
			\scntext{примечание}{\textit{n} --- размер обучающей выборки, $Y\underscore{i}$ --- эталонное значение функции, $\widetilde{Y\underscore{i}}$ --- результат, полученный НС}
		\end{scnindent}
	\scnfileitem{
		\begin{equation*}
		CE = - \frac{1}{n} \sum\underscore{i=1}\upperscore{n} (Y\underscore{i}\log(\widetilde{Y\underscore{i}}) + (1-Y\underscore{i})\log(1 - \widetilde{Y\underscore{i}}))
		\end{equation*}}
		\begin{scnindent}
			\scntext{примечание}{\textit{n} --- размер обучающей выборки, $Y\underscore{i}$ --- эталонное значение функции, $\widetilde{Y\underscore{i}}$ --- результат, полученный НС.
				\\(случай 2-классовой классификации)}
		\end{scnindent}
	\scnfileitem{
		\begin{equation*}
			CE = - \frac{1}{n} \sum\underscore{i=1}\upperscore{n} \sum\underscore{c=1}\upperscore{M} Y\underscore{i}\upperscore{c} \log{\widetilde{Y}\underscore{i}\upperscore{c}}
		\end{equation*}}
		\begin{scnindent}
			\scntext{примечание}{случай многоклассовой классификации}
		\end{scnindent}
	\scnfileitem{В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия выбора минимизируемой функции ошибки}}.}
\end{scnrelfromvector}
	
\scnheader{Начальная инициализация параметров нейронной сети}
\scntext{пример}{Наиболее часто используемые варианты инициализации весовых коэффициентов и порогов нейронной сети}
\begin{scnindent}
	\begin{scnrelfromset}{разбиение}
		\scnfileitem{Инициализация значениями из равномерного распределения на каком-то небольшом интервале, например,  $\scnleftsquarebrace$ -0.1, 0.1$\scnrightsquarebrace$.}
		\scnfileitem{Инициализация значениями из стандартного нормального распределения.}
		\scnfileitem{Инициализация по методу Ксавье.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Glorot2010}}
			\end{scnrelfromset}
			\scntext{примечание}{Применяется для предотвращения резкого уменьшения или увеличения значений выхода нейронных элементов после применения функции активации при прямом прохождении образа через глубокую нейронную сеть. Фактически инициализация этим методом осуществляется посредством выбора значений из равномерного распределения на отрезке $\scnleftsquarebrace - \sqrt{6} / \sqrt{n\underscore{i}+n\underscore{i+1}}, \sqrt{6} / \sqrt{n\underscore{i}+n\underscore{i+1}}\scnrightsquarebrace $, где $n\underscore{i}$ --- это число входящих связей в данный слой, а $n\underscore{i}$ --- число исходящих связей из данного слоя. Таким образом, инициализация этим методом проводится для разных слоев нейронной сети из разных отрезков.}
		\end{scnindent}
		\scnfileitem{Инициализация, полученная из предобученной модели.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Glorot2010}}
			\end{scnrelfromset}
			\scntext{примечание}{Вариант инициализации, который предполагает использование в качестве \scnqq{стартовой} модели предобученной модели, взятой из некоторого репозитория предобученных моделей, обученную самим исследователем или в процессе работы интеллектуальной системы.}
		\end{scnindent}
		\scnfileitem{Инициализация по методу Кайминга.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{He2015}}
			\end{scnrelfromset}
			\scntext{примечание}{Данный метод инициализации применяется для решения проблемы \scnqq{затухающего} градиента и \scnqq{взрывающегося} градиента. Производится посредством выбора значений из равномерного распределения на отрезке: 
				\begin{equation*}
					\scnleftsquarebrace -\sqrt{2} / \sqrt{(1+a\upperscore{2})fan}, \sqrt{2} / \sqrt{(1+a\upperscore{2})fan}\scnrightsquarebrace
				\end{equation*}
				где \textit{a} --- угол наклона к оси абсцисс для отрицательной части области определения функции активации типа ReLU (для обычной ReLU функции этот параметр равен 0), $fan$ --- параметр режима работы, который для фазы прямого распространения равен количеству входящих связей (для устранения эффекта \scnqq{взрывающегося} градиента), а для фазы обратного распространения --- количеству выходящих (для устранения эффекта \scnqq{затухающего} градиента).
				\\В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия начальной инициализации и.н.с.}}.}
		\end{scnindent}
	\end{scnrelfromset}
\end{scnindent}

\scnheader{Выбора гиперпараметров и.н.с.}
\scntext{примечание}{На практике некоторые гиперпараметры (такие как количество слоев, их типы, количество нейронов в слое) часто определяются экспериментально, в процессе итеративного поиска лучшего варианта решения задачи. Хотя способы частично автоматизировать этот процесс существуют, они все же рассчитаны на наличие некоторых предусловий проведения эксперимента, в частности интервалов изменения параметра (например, скорости обучения).
	\\К гиперпараметрам, подбираемым на этом этапе, относятся:
	\begin{itemize}
		\item параметры обучения \textit{и.н.с.} (скорость обучения, моментный параметр, размер мини-батча);
		\item архитектура модели \textit{и.н.с.}, опирающаяся на ранее сформулированные спецификации входных и выходных данных (например, количество нейронов в определенном слое (слоях) или конфигурации целых слоев).
	\end{itemize}
	Нахождение оптимальных гиперпараметров может быть получено, например, использованием метода сеточного поиска, который позволяет проверить значения гиперпараметров, взятые с определенным шагом или из определенного интервала (кортежа). С помощью этого метода выбирается оптимальный набор гиперпараметров, который дает лучшие результаты, он используется для последующего дообучения. Или же, если полученные результаты являются приемлемыми, то процесс дальнейшего обучения вообще не проводится. Следует отметить затратность данного метода, так как фактически осуществляется перебор различных значений параметров обучения. Для снижения объема работы применяется метод случайного поиска.
	\\Для оптимизации архитектуры определяются типы слоев нейронной сети, количество нейронных элементов в каждом слое, их характеристики --- функция активации, для сверточных элементов --- размер ядра, а также параметры padding и шаг свертки (stride).
	\\Здесь же может осуществляться оценка не только пользовательского варианта сети, но и предобученной архитектуры. Основное правило при выборе --- количество параметров модели не должно превышать размер обучающей выборки. Для предобученных архитектур это ограничение снимается.
	\\В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия выбора гипперпараметров и.н.с.}}. Действие использует классификацию и спецификации гиперпараметров \textit{и.н.с.}}

\scnheader{Обучение модели на обучающей выборке}
\scntext{примечание}{Производится обучение модели до достижения выбранной точности (оценивается на тестовой выборке) или по другим заданным критериям (достижение заданного количества эпох обучения, неизменность точности на протяжении заданного количества эпох, падение точности на валидационной выборке и так далее).}
\scntext{примечание}{В интеллектуальной среде проектирования данный этап соответствует выполнению \textbf{\textit{действия обучения и.н.с.}}. Действие обучения \textit{и.н.с.} --- действие, в ходе которого реализуется определенный метод обучения \textit{и.н.с.} с заданными параметрами обучения \textit{и.н.с.}, методом оптимизации и функцией потерь.
	\\При обучении возможно возникновение следующих проблем:
	\begin{itemize}
		\item \textit{переобучение} --- проблема, возникающая при обучении \textit{и.н.с.}, заключающаяся в том,
		что сеть хорошо адаптируется к паттернам входной активности из обучающей выборки, при этом теряя способность к обобщению.
		Переобучение возникает из-за применения неоправданно сложной модели при обучении \textit{и.н.с.} Это происходит,
		когда количество настраиваемых параметров \textit{и.н.с.} намного больше размера обучающей выборки. Возможные
		варианты решения проблемы заключаются в упрощении модели, увеличении выборки, использовании регуляризации
		(параметр регуляризации, техника dropout и так далее).\\
		Обнаружение переобученности сложнее, чем недообученности. Как правило, для этого применяется
		кросс-валидация на валидационной выборке, позволяющая оценить момент завершения процесса обучения.
		Идеальным вариантом является достижение баланса между переобученностью и недообученностью.
		\item \textit{недообучение} --- проблема, возникающая при обучении  \textit{и.н.с.}, заключающаяся в том,
		что сеть дает одинаково плохие результаты на обучающей и контрольной выборках.
		Чаще всего такого рода проблема возникает при недостаточном времени, затраченном на обучение модели.
		Однако это может быть вызвано и слишком простой архитектурой модели либо малым размером обучающей
		выборки. Соответственно решение, которое может быть принято ML-инженером, заключается в устранении
		этих недостатков: увеличение времени обучения, использование модели с большим числом настраиваемых
		параметров, увеличение размера обучающей выборки, а также уменьшение регуляризации и более тщательный
		отбор признаков для обучающих примеров.
	\end{itemize}}

\scnheader{метод обучения и.н.с.}
\scnsubset{метод}
\scnrelfrom{разбиение}{Классификация алгоритмов обучения}
\begin{scnindent}
	\begin{scneqtoset}
		\scnitem{метод обучения с учителем}
		\begin{scnindent}
			\scntext{пояснение}{\textbf{\textit{метод обучения с учителем}} --- метод обучения с использованием заданных целевых переменных.}
			\scnsuperset{метод обратного распространения ошибки}
			\begin{scnindent}
				\scnidtf{м.о.р.о.}
				\scntext{пояснение}{м.о.р.о. использует заданный метод оптимизации и заданную функцию потерь для реализации фазы обратного распространения ошибки и изменения настраиваемых параметров и.н.с. Одним из самых распространенных	методов оптимизации является метод стохастического градиентного спуска.}
				\scntext{пояснение}{Следует также отметить, что несмотря на то, что метод отнесен к методам обучения с учителем, в случае	использования м.о.р.о. для обучения автокодировщиков в классических публикациях он рассматривается как	метод обучения без учителя, поскольку в данном случае размеченные данные отсутствуют.}
			\end{scnindent}
		\end{scnindent}
		\scnitem{метод обучения без учителя}
		\begin{scnindent}
			\scntext{пояснение}{\textbf{\textit{метод обучения без учителя}} --- метод обучения без использования заданных целевых переменных(в режиме самоорганизации)}
			\scntext{пояснение}{В ходе выполнения алгоритма метода обучения без учителя выявляются полезные структурные свойства набора. Неформально его понимают как метод для извлечения информации из распределения, выборка для которого	не была вручную аннотирована человеком.}
			\begin{scnindent}
				\begin{scnrelfromset}{источник}
					\scnitem{\scncite{Goodfellow2017}}
				\end{scnrelfromset}
			\end{scnindent}
		\end{scnindent}
	\end{scneqtoset}
\end{scnindent}

\scnheader{метод обучения \textit{и.н.с.}}
\scntext{определение}{метод обучения \textit{и.н.с.} --- это процесс итеративного поиска оптимальных значений настраиваемых параметров \textit{и.н.с.}, минимизирующих некоторую заданную функцию потерь.}
\scntext{примечание}{Стоит отметить, что хотя целью применения метода обучения является минимизация функции потерь, \scnqq{полезность} полученной после обучения модели можно оценить только по достигнутому уровню ее обобщающей способности.}
\scntext{примечание}{\\Методы обучения могут быть поделены на две большие группы --- \textit{\textbf{методы обучения с учителем}} и \textit{\textbf{методы обучения без учителя}} (контролируемый и неконтролируемый методы обучения).
	\\\textit{метод обучения с учителем} --- метод обучения с использованием заданных целевых переменных.
	\\Одним из методов обучения с учителем является метод обратного распространения ошибки.}

\scnheader{метод обратного распространения ошибки}
\scntext{описание}{Приведем его описание в виде алгоритма:
	\begin{algorithm}[H]
		\KwData{$X$ --- данные, $E\underscore{t}$ --- желаемый отклик (метки), $E\underscore{m}$ --- желаемая ошибка (в соответствии с выбранной функцией потерь)}
		\KwResult{обученная нейронная сеть \textit{Net}}
		инициализация весов \textit{W} и порогов \textit{T};\\
		\Repeat{$E<E\underscore{m}$}{
			\ForEach{$x \in X$, $e \in E\underscore{t}$}{
				фаза прямого распространения сигнала: вычисляются активации для всех слоев и.н.с.;\\
				фаза обратного распространения ошибки: вычисляются ошибки для последнего слоя и всех предшествующих слоев;\\
				изменение настраиваемых параметров и.н.с. в соответствии с вычисленными ошибками;\\
			}
			вычисление общей ошибки E на данной эпохе;
		}
	\end{algorithm}
	\textit{метод обратного распространения ошибки} использует заданный метод оптимизации и заданную функцию потерь для реализации фазы обратного распространения ошибки и изменения настраиваемых параметров и.н.с. Одним из самых распространенных методов оптимизации является метод стохастического градиентного спуска. Приведенный метод используется для реализации последовательного варианта обучения.}
\scntext{примечание}{Следует также отметить, что несмотря на то, что метод отнесен к методам обучения с учителем, в случае его использования для обучения автокодировщиков в классических публикациях он рассматривается как метод обучения без учителя, поскольку в данном случае размеченные данные отсутствуют.}


\scnheader{метод обучения без учителя}
\scniselement{метод обучения}
\scntext{определение}{метод обучения без учителя --- это метод обучения без использования заданных целевых переменных (в режиме самоорганизации)}
\scntext{примечание}{В ходе выполнения алгоритма метода обучения без учителя выявляются полезные структурные свойства набора. Неформально его понимают как метод для извлечения информации из распределения, выборка для которого не была вручную аннотирована человеком. Метод обучения без учителя может рассматриваться как вспомогательный метод для начальной инициализации настраиваемых параметров и.н.с. В этом случае он является методом предобучения.}
\begin{scnindent}
	\begin{scnrelfromset}{источник}
		\scnitem{\scncite{Goodfellow2017}}
	\end{scnrelfromset}
\end{scnindent}

\scnheader{целевая функция}
\begin{scnrelfromvector}{методы оптимизации}
		\scnfileitem{SGD (стохастический градиентный спуск). В данном методе корректировка настраиваемых параметров и.н.с. выполняется в направлении максимального уменьшения функции стоимости, то есть в направлении, противоположном вектору градиента функции потерь.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Golovko2017}}
				\scnitem{\scncite{Haykin2006}}
			\end{scnrelfromset}
		\end{scnindent}
		\scnfileitem{Метод Нестерова. Обучение методом стохастического градиентного спуска не редко происходит очень медленно. Импульсный метод позволяет ускорить обучение, особенно в условиях высокой кривизны, небольших, но устойчивых градиентов или зашумленных градиентов. В импульсном методе вычисляется экспоненциально затухающее скользящее среднее прошлых градиентов и продолжается движение в этом направлении. Метод Нестерова является вариантом импульсного алгоритма, в котором градиент вычисляется после применения текущей скорости.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Goodfellow2017}}
			\end{scnrelfromset}
		\end{scnindent}
		\scnfileitem{AdaGrad: данный метод по отдельности адаптирует скорости обучения всех настраиваемых параметров и.н.с., умножая их на коэффициент, обратно пропорциональный квадратному корню из суммы всех прошлых значений квадрата градиента.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Duchi2011}}
			\end{scnrelfromset}
		\end{scnindent}
		\scnfileitem{RMSProp. Данный метод является модификацией AdaGrad, которая позволяет улучшить его поведение в невыпуклом случае путем изменения способа агрегирования градиента на экспоненциально взвешенное скользящее среднее. Использование экспоненциально взвешенного скользящего среднего гарантирует повышение скорости сходимости после обнаружения выпуклой впадины, как если бы внутри этой впадины алгоритм AdaGrad был инициализирован заново}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Goodfellow2017}}
			\end{scnrelfromset}
		\end{scnindent}
		\scnfileitem{Adam. Данный метод можно рассматривать как комбинацию RMSProp и AdaGrad. Помимо усредненного первого момента, данный метод использует усредненное значение вторых моментов градиентов.}
		\begin{scnindent}
			\begin{scnrelfromset}{источник}
				\scnitem{\scncite{Kingma2014}}
			\end{scnrelfromset}
		\end{scnindent}
\end{scnrelfromvector}
\scntext{примечание}{Отметим, что успешность применения методов оптимизации зависит главным образом от знакомства пользователя с соответствующим алгоритмом.}
\begin{scnindent}
	\begin{scnrelfromset}{источник}
		\scnitem{\scncite{Goodfellow2017}}
	\end{scnrelfromset}
\end{scnindent}

\scnheader{функция потерь}
\scntext{примечание}{важный компонент, влияющий на процесс обучения нейросетевой модели}
\scntext{определение}{функция потерь --- это функция, используемая для вычисления ошибки, рассчитываемой как разница между фактическим эталонным значением и прогнозируемым значением, получаемым \textit{и.н.с.}}
\begin{scnrelfromvector}{примечание}
	\scnfileitem{Среди функций потерь, используемые в качестве целевых функций для применяемого метода оптимизации, можно выделить MSE, BCE, MCE}
	\scnfileitem{MSE --- средняя квадратичная ошибка}
	\begin{scnindent}
		\scnrelfrom{формула}{
			\begin{equation*}
				MSE = \frac{1}{L} \sum\underscore{l=1}\upperscore{L} \sum\underscore{i=1}\upperscore{m} (y\underscore{i}\upperscore{l} - e\underscore{i}\upperscore{l})\upperscore{2}
			\end{equation*}}
		\begin{scnindent}
			\scntext{примечание}{$y\underscore{i}\upperscore{l}$ --- прогноз модели, $e\underscore{i}\upperscore{l}$ --- ожидаемый (эталонный) результат, \textit{m} --- размерность выходного вектора, \textit{L} --- объем обучающей выборки}
		\end{scnindent}
	\end{scnindent}
	\scnfileitem{BCE --- бинарная кросс-энтропия (binary cross-entropy)}
	\begin{scnindent}
		\scnrelfrom{формула}{
			\begin{equation*}
				BCE = - \sum\underscore{l=1}\upperscore{L} (e\upperscore{l} \log(y\upperscore{l}) + (1 - e\upperscore{l})\log(1 - y\upperscore{l}))
			\end{equation*}}
		\begin{scnindent}
			\scntext{примечание}{$y\upperscore{l}$ --- прогноз модели, $e\upperscore{l}$ --- ожидаемый (эталонный) результат: \textit{0} или \textit{1}, \textit{L} --- объем обучающей выборки}
		\end{scnindent}
	\end{scnindent}
	\scnfileitem{MCE --- мультиклассовая кросс-энтропия (multiclass cross-entropy)}
	\begin{scnindent}
		\scnrelfrom{формула}{
			\begin{equation*}
				MCE = - \sum\underscore{l=1}\upperscore{L} \sum\underscore{i=1}\upperscore{m} e\underscore{i}\upperscore{l \log(y\underscore{i}\upperscore{l})}
			\end{equation*}}
		\begin{scnindent}
			\scntext{примечание}{$y\underscore{i}\upperscore{l}$ --- прогноз модели, $e\underscore{i}\upperscore{l}$ --- ожидаемый (эталонный результат), \textit{m} --- размерность выходного вектора}
		\end{scnindent}
	\end{scnindent}
\end{scnrelfromvector}

\scnheader{бинарная кросс-энтропия}
\scnidtf{BCE}
\scntext{примечание}{Отметим, что для бинарной кросс-энтропии в выходном слое \textit{и.н.с.} будет находиться один нейрон, а для для мультиклассовой кросс-энтропии количество нейронов в выходном \textit{слое и.н.с.} совпадает с количеством классов.}

\scnheader{задача классификации}
\scntext{примечание}{Для решения задачи классификации рекомендуется использовать бинарную или мультиклассовую кросс-энтропийную функцию потерь, для решения задачи регрессии рекомендуется использовать среднюю квадратичную ошибку.}

\scnheader{SCg-текст. Действие обучения и.н.с.}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/ann_training_nn_scg.png}}
\begin{scnindent}
	\scntext{примечание}{Пример действия обучения \textit{и.н.с.}}
\end{scnindent}

\scnheader{Оценка эффективности и.н.с}
\scntext{примечание}{После выполнения обучения осуществляется оценка полученной модели с помощью метрик оценки качества.
	\\Далее результат оценки может быть визуализирован с помощью матрицы ошибок (confusion matrix) и ROC-кривой.}
\scntext{примечание}{В интеллектуальной среде проектирования данный этап соответствует выполнению \textit{действия оценки эффективности и.н.с.}.}

\scnheader{матрица ошибок}
\scntext{определение}{матрица ошибок --- это матрица, в которую помещены сведения о числе истинно-положительных, истинно-отрицательных, ложно-положительных и ложно-отрицательных предсказаниях классификатора.}
\scnrelfrom{смотрите}{Рисунок. Матрица ошибок}

\scnheader{Рисунок. Матрица ошибок}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/conf_matrix.png}}

\scnheader{ROC-кривая}
\scnidtf{receiver operating characteristic}
\scntext{определение}{ROC-кривая --- это график, в котором, основываясь на заданном пороге решения классификатора, рассчитываются доли ложноположительных и истинно положительных исходов. Основываясь на ROC-кривой, высчитывается AUC-показатель (площадь под кривой), которая используется в качестве характеристики качества модели.}

\scnheader{Задача. Классификация цифр из выборки рукописных цифр MNIST}
\begin{scnrelfromvector}{пример}
	\scnfileitem{Рассмотрим пример выполнения описанных этапов разработчиком для конкретной задачи --- \textit{классификации цифр из выборки рукописных цифр MNIST}:
		\begin{itemize}
		\item Исходными данными задачи является: выборка из 70.000 изображений, предварительно разделенная на обучающую (60.000 изображений) и контрольную (10.000 изображений) выборки. Каждое изображение представлено двумерным массивом 28x28 чисел из интервала $\scnleftsquarebrace  0, 255\scnrightsquarebrace$, числа представляют определенный оттенок серого цвета. Помимо этого каждому изображению соответствует метка класса, соответствующая конкретной цифре от 0 до 9.
		\\Ставится задача: \textit{обучить модель, которая будет принимать на вход двумерный массив данных и возвращать метку класса, соответствующей распознанной цифре.}
		\\Таким образом, тип решаемой задачи --- \textbf{классификационная}, природа данных задачи --- \textbf{изображения}.
		\item В рассматриваемой выборке отсутствуют аномалии, ошибочные данные, признаки с отсутствующими значениями.
		\item В рассматриваемой задаче отсутствуют несодержательные признаки.
		\item В качестве метода предобработки данных используем масштабирование признаков, а именно нормализацию на отрезок $\scnleftsquarebrace 0, 1\scnrightsquarebrace$.
		\item Выполним разбиение обучающей части данных на обучающую и валидационную выборки в соотношении 4:1 (48.000 в обучающей и 12.000 в валидационной).
		\item Так как выборка включает в себя изображения, будем использовать сверточную нейронную сеть.
		\item Не требуется.
		\item В качестве оптимизационного алгоритма будем использовать метод стохастического градиентного спуска (SGD).
		\item Так как решается задача классификации, выберем в качестве минимизируемой функции кросс-энтропийную функцию потерь.
		\item В качестве начальной инициализации будем использовать инициализацию по методу Кайминга.
		\item На предыдущих этапах было определено, что для решения задачи будет использоваться сверточная нейронная сеть. При использовании one-hot кодирования в последнем полносвязном слое будет 10 нейронов по числу классов в задаче.
		\end{itemize}}
	\scnfileitem{Для упрощения будем использовать архитектуру, изображенную на \textit{Рисунок. Архитектура и.н.с., решающая задачу классификации цифр}, не содержащую промежуточные слои.}
	\scnfileitem{Для нахождения оптимального набора гиперпараметров будем применять метод случайного поиска.
		\\Перечислим кортежи, из которых будут сэмплироваться гиперпараметры:
		\begin{itemize}
			\item Скорость обучения --- (0.9, 0.1, 0.01, 0.001);
			\item Количество нейронов в сверточном слое --- (5, 10, 15, 20);
			\item Размер ядра свертки --- (3, 5, 7, 9);
			\item Моментный параметр --- (0, 0.5, 0.9);
			\item Размер мини-батча --- (16, 32, 64, 128).
		\end{itemize}}
	\scnfileitem{После определения данных параметров и оценки эффективности работы алгоритма, получим следующую таблицу: \textit{Таблица. Результаты решения задачи}}
	\scnfileitem{Можно заметить, что лучший результат (acc = 0.9839) по обобщающей способности на валидационной выборке был получен при следующих параметрах: mbs = 64, ks = 7, lr = 0.01, momentum = 0.9, cnc = 15.
		\begin{itemize}
		\item В качестве критерия останова нами был выбран самый простой критерий по достижению заданного количества эпох обучения. Дообучение не проводилось, для оценки обобщающей способности использовалась модель, полученная после выполнения процедуры подбора гиперпараметров. Обобщающая способность на тестовой выборке составила \textbf{0.9853}, то есть \textbf{98.53\percent}.
		\item Построив матрицу ошибок на основании обученной модели и тестовой выборки, получим результат, проиллюстрированный на рис. \textit{Рисунок. Матрица ошибок для задачи MNIST}
		\end{itemize}
		Мы получили матрицу с явно выраженным диагональным преобладанием, таким образом полученная модель делает относительно небольшое число ошибок.}
\end{scnrelfromvector}

\scnheader{Рисунок. Архитектура и.н.с., решающая задачу классификации цифр}
\scnrelfrom{описание примера}{\scnfileimage[20em]{Contents/part_ps/src/images/sd_ps/sd_ann/model.png}}

\scnheader{Таблица. Результаты решения задачи}
\scnrelfrom{описание примера}{\scnfileimage[20em]{Contents/part_ps/src/images/sd_ps/sd_ann/results_table.png}}
\begin{scnindent}
	\scntext{пояснение}{Используемые сокращения: mbs --- mini-batch size, ks --- kernel size, lr --- learning rate, cnc --- convolutional neurons count, acc --- accuracy, it --- iterations count}
\end{scnindent}

\scnheader{Рисунок. Матрица ошибок для задачи MNIST}
\scnrelfrom{описание примера}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/conf_matrix_result}}

\scnheader{действие по построению и.н.с.}
\scntext{примечание}{Исходя из анализа этапов построения и.н.с., которые выполняют разработчики, можно вывести следующую классификацию действий по построению и.н.с.}
\begin{scnrelfromset}{декомпозиция}
	\scnitem{действие по обработке выборки}
	\begin{scnindent}
		\begin{scnrelfromset}{декомпозиция}
			\scnitem{действие поиска подходящей обучающей выборки}
			\scnitem{действие формирования требований к обучающей выборке}
			\scnitem{действие очистки выборки}
			\scnitem{действие выявления содержательных признаков}
			\scnitem{действие трансформации выборки}
			\scnitem{действие разбиения выборки}
		\end{scnrelfromset}
	\end{scnindent}
	\scnitem{действие по проектированию и.н.с.}
	\begin{scnindent}
		\begin{scnrelfromset}{декомпозиция}
			\scnitem{действие выбора класса нейросетевых методов}
			\scnitem{действие формирования спецификации входов и выходов и.н.с.}
		\end{scnrelfromset}
	\end{scnindent}
	\scnitem{действие обучения и.н.с.}
	\begin{scnindent}
		\begin{scnrelfromset}{декомпозиция}
			\scnitem{действие выбора метода оптимизации}
			\scnitem{действие выбора минимизируемой функции ошибки}
			\scnitem{действие начальной инициализации и.н.с.}
			\scnitem{действие выбора гиперпараметров и.н.с.}
			\scnitem{действие обучения и.н.с.}
			\scnitem{действие оценки эффективности и.н.с.}
		\end{scnrelfromset}
	\end{scnindent}
\end{scnrelfromset}

\scnheader{темпоральность нейронной сети}
\scntext{примечание}{Так как в результате действий по построению \textit{и.н.с.} объект этих действий, конкретная \textit{и.н.с.}, может существенно меняться (меняется конфигурация сети, ее весовые коэффициенты), то \textit{и.н.с.} представляется в базе знаний как темпоральное объединение всех ее версий. Каждая версия является \textit{и.н.с.} и темпоральной сущностью. На множестве этих темпоральных сущностей задается темпоральная последовательность с указанием первой и последней версии. Для каждой версии описываются специфичные знания.
	\\Общие для всех версий знания описываются для \textit{и.н.с.}, являющейся темпоральным объединением всех версий (рисунок \textit{SCg-текст. Темпоральность нейронной сети})}

\scnheader{SCg-текст. Темпоральность нейронной сети}
\scnrelfrom{описание примера}{\scnfileimage[20em]{Contents/part_ps/src/images/sd_ps/sd_ann/temporal_neural_network_scg.png}}

\end{scnsubstruct}

\begin{scnrelfromvector}{заключение}
	\scnfileitem{В главе описан подход к \textit{интеграции и конвергенции искусственных нейронных сетей с базами знаний} в \textit{интеллектуальных компьютерных системах нового поколения} с помощью представления и интерпретации \textit{искусственной нейронной сети} в \textit{базе знаний}.}
	\scnfileitem{Описаны \textit{Синтаксис, Денотационная и Операционная семантика Языка представления нейросетевых методов в базах знаний}, который позволяет представить и интерпретировать в памяти интеллектуальной системы любую \textit{и.н.с.} Наличие такого языка порождает семантическую совместимость нейросетевого метода с другими методами, представленными в памяти системы, что позволяет анализировать саму \textit{и.н.с.} и этапы ее работы любыми другими методами системы.}
	\scnfileitem{Так же наличие языка представления нейросетевых методов позволяет описывать в памяти системы экспертные знания разработчиков \textit{и.н.с.} В главе приведены этапы построения \textit{и.н.с.}, которые выполняют разработчики \textit{и.н.с.} На основании этих этапов, c целью проектирования интеллектуальной среды построения \textit{нейросетевых методов}, в \textit{базе знаний} были классифицированы и описаны действия по построению \textit{и.н.с.}}
	\scnfileitem{Проектирования и реализация интеллектуальной среды построения \textit{и.н.с.} в \textit{базе знаний} системы является одним из двух основных направлений дальнейшего развития работу по конвергенции и интеграции и.н.с. с базами знаний.}
	\scnfileitem{Вторым основным направлением является разработка подхода к обработке фрагментов \textit{базы знаний} с помощью \textit{и.н.с.}, для чего необходимо разработать универсальный алгоритм взаимно-однозначного соответствия фрагментов базы знаний и входных векторов \textit{и.н.с.} Язык представления знаний способен представить любое знание. Наличие в системе нейросетевого метода, способного принимать на вход фрагменты знаний, позволит решить новые, слабо изученные классы задач.}
\end{scnrelfromvector}


\bigskip
\scnendcurrentsectioncomment
\end{scnsubstruct}
\end{SCn}
